
분석 목표 - 업무의 목적, 이유, 비즈니스에 미치는 영향, 구체적인 설계와 지표, 일정과 예상 Output

    NS SHOP+ 판매실적 예측을 통한 편성 최적표 방안(모형) 도출
    NS SHOP+ 편성데이터(NS홈쇼핑)를 활용하여 방송편성표에 따른 판매실적을 예측하고,
    최적 수익을 고려한 요일별/시간대별/카테고리별 편성 최적화 방안(모형) 제시

    수집가능한 데이터의 정의 -> 주어진 데이터를 이해 및 파악하기 + 외에 더 필요하다고 판단되는 데이터는 무엇인지

    -> 가장 중요한 KPI는 취급액(총 판매액)
    -> 만족도는 주요 관심사가 아닌가?
    -> 방송 날자별, 노출시간별, 상품군별, 그중에서도 상품별, 판매 단가별의 총 판매액이 궁금하다.
    -> 날짜 노출시간 별로 시청률이 높을때, 실제 판매액사이의 관계를 알아보고자 한다.
    -> 방송이 연속된 경우, (같은 카테고리 반복, 다른 카테고리 방송) 의 영향
    -> 같은 상품에 대해서 반복된 반복을 하는 경우, 언제 취급액이 가장 놓은지
    -> 전반적인 시청률과 취급액 사이의 관계, 그리고 시청률은 높지만 취급액이 높지 않은 품목이 있다면?

    -> 동시간대에 판매된 목록들에 대해서는 어떤 제품이 많이 팔리는지.
        예를들어 남성제품과 여성제품의 비교

    1) 방송 편성표에 따른 판매실적을 예측하는 모델의 생성. -> 일시, 노출, 상품군, 단가 별로의 취급액을 예측한다.
    2) 수익의 극대화를 위해 요일별, 시간대별, 카테고리 별로 어떤 상품을 배치를 해야 하는지 모형을 제시한다.

        ★참고사항

        1. 취급액 =판매단가 X 주문량(취소,반품제외)
        2. 판매가  0원(무형상품)은 추정 제외
        3. 매주 토요일  18:00~18:20은 정보방송시간으로 추정 제외(당사사정에 따라 ±20 편성 변경 있음)
        4. 편성 듀레이션은  10분~60분내외 임
        5. 2020.06월 편성표는 월~토 6:20~2:20(익일), 일요일은 6:20~2:00로 구성 (심야시간대 제외)
        단, raw data의 19.01.01~19.10.31은 6:00~2:00로 운영하였음 11.01부터 6:20~2:20운영

데이터의 수집 - 올바른 데이터의 선택

    -> 분석에 필요한 데이터를 정의

    2019년 실적 데이터 - 방송일시, 노출(분) - 방송시간, 마더코드, 상품코드, 상품명, 상품군, 판매단가, 취급액
    2019년 시청률 데이터 - 방송날짜, 시간별(1분간격) 시청률

    -> 데이터를 기반으로 알아낼 수 있는 것이 무엇인가? -> 시각화를 통해 대략적인 관계를 파악해보자.

데이터 전처리

    -> 수집한 데이터에 존재하는 결측값이나 오류를 수정/보완한다. 필요에 따라 구조나 특성을 변경

    중복값 제거, 결측값 보정, 데이터 연계/통합합

모델 생성 - 모델의 피팅, 검증

    -> 다양한 관점을 반영
    -> 관련 테이블 간의 관계 설정

    - 어떻게 테이블을 나누고 테이블 간의 관계를 연결할 것인지.
    - 핵심 사건(사건, 거래)과 차원(일시, 장소) 의 기록으로 나눈다.

    -> 모델 평가용 데이터가 따로 존재. 2020-06

데이터 분석 및 시각화

    탐색적 데이터 분석 - 그룹별로의 통계치 확인, 분포의 확인
    확증적 데이터 분석
    통계 분석 - 가설검증 및 모수의 추정, 변수간의 상관관계 파악, 통계 모형의 구축, 차원의 축소(요인분석, 군집분석)
    머신러닝 - 분류 및 회귀 문제(지도), 이상치 탐색, 클러스터링(비지도)

    -> 요약 설명에는 기술통계를 주로 사용.

    인사이트 도출?

//////////////////////////////////////////////////////////////////////////////////////////////////////////

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime as dt

data = pd.read_excel('training_data/data_analysis_performance.xlsx', header = 1, parse_dates=True)
rate = pd.read_excel('training_data/data_analysis_rating.xlsx', header = 1, index_col = 0, parse_dates=True)

data.head()

-> 데이터를 성공적으로 호출 하였다.
첫행은 제목이 쓰여 있으므로 두번째 행이 컬럼명이 된다. data 데이터프에밍에 인덱스행은 첫열에 생성하였다.
-> 원래는 인덱스 행을 첫열로 사용하려 했으나, 중복된 값들이 존재하여 따로 생성하였다.

rate 데이터프레임은 인덱스행을 설정하지 않고 1열로 사용하였다.

다양한 관점에서 분석을 시작해 보자.

//////////////////////////////////////////////////////////////////////////////////////////////////////////

데이터의 이해. 어떤 종류의 데이터들이 존재하는지 먼저 확인해보자.

data.info()를 통해 데이터 타입을 확인해 보자.

- performance DataFrame

    #   Column  Non-Null Count  Dtype
    ---  ------  --------------  -----
     0   방송일시    38309 non-null  datetime64[ns]
     1   노출(분)   21525 non-null  float64
     2   마더코드    38309 non-null  int64
     3   상품코드    38309 non-null  int64
     4   상품명     38309 non-null  object
     5   상품군     38309 non-null  object
     6   판매단가    38309 non-null  int64
     7   취급액     37372 non-null  float64

    총 38309개의 데이터가 존재하는데, 이중 노출과 취급액에는 생략된 값들이 존재한다.

    타입은 다음과 같다
    일시는 datetime, 노출분과 취급액은 float형,
    판매단가, 마더코드, 상품코드는 정수,
    상품군과 상품명은 오브젝트 타입이다.

    -> index - '방송일시', '노출(분)', '마더코드', '상품코드', '상품명', '상품군', '판매단가', '취급액'

- rate DataFrame

    Columns: 367 entries, 시간대 to 2019-01-01 to 2019-12-31
    dtypes: float64(366), object(1)

    float데이터 타입.

    -> 날짜별, 시간별 시청률

----------------------------------------------------------------------------------------------------------

먼저 결측치의 여부를 확인해 보자. -> isnull(), isnull.sum을 통해 결측지의 여부를 확인할 수 있다.

-> 상품군이 무형인 경우에 판매가 및 취급액이 null인것을 확인할 수 있다.
-> 노출(분)의 경우에는 16784개의 결측값이 존재하며, 취급액에는 937개의 결측치가 존재한다.

노출(분)의 결측값은 이전값들을 받아서 결측값을 채우면 되는 경우이며,

취급액의 결측치는 모두가 상품군이 무형인 것인지 확인해야 한다.
-> 무형의 상품군을 count해 본다.

data[data['상품군']=='무형'].count()를 시행해 본 결과 937개의 데이터가 나왔다.

-> 따라서 모든 취급액의 결측치가 무형인 것으로 드러났다.
-> 무형을 제외한 데이터프레임을 생성한다.
data_no_zero = data[data['상품군']!='무형']

이제는 노출(분)의 결측값을 이전값을 받도록 해준다.

fillna() 를 사용한다.
    fillna(0)을 한다면 NaN을 0으로 채운다.
    (method='pad')를 사용하면 이전값으로 채운다. / ffill
    (method='bfill')을 사용하면 뒤의값으로 채운다. / backfill

data_filtered에 저장해 보자.

data_filtered = data_no_zero.fillna(method='pad')

----------------------------------------------------------------------------------------------------------

각 컬럼별로의 데이터 분석을 해보자.

컬럼별로 데이터의 수 -> groupby를 통해서 분류를 해보고, 통계치를 살펴보자.

data_filtered.groupby('노출(분)').count()를 통해 살펴본 결과,

-> fillna()를 통해 들어간 값들이 정수가 아닌 수들이 입력되었다.
    -> 해결해야한다...

-> 처음에 데이터를 불러올 시에 null값에 대한 정의를 해주어야 하는거 같다.
-> 다시 처음에 데이터 호출부터 시작!

-> null값들은 default로 NaN으로 입력이 잘 된다.

노출(분)의 데이터들이 정수형이 아닌 이유는 excel상에서는 반올림 되어 표시되어있다.
    2.4667 -> 2
    2.18667 -> 3
    3.2833 ->  3

-> 그렇다면 소숫점 아래의 숫자들에 대해서는 생각을 안하고 분단위의 데이터만 분석할 것인가?
-> 이러한 방식이 노출분이이 클 에는 의미가 작지만 노출분이 작을 때에는 크지 않을까??

-> 노출(분)의 값을 반올림하여 데이터를 보자.
data_filtered['노출(분)'] = round(data_filtered['노출(분)'])

----------------------------------------------------------------------------------------------------------

***** 여태까지 같은 시간대에 방송한 다른 품목에 대해서, 다르다고 count를 하였는데
이를 한 방송으로 보는것이 맞는것인가? 아니면 다른 방송으로 보는 것이 맞는것인가?

***** 같은 시간에 방송한 항목들에 대해서 어떻게 고려할 것인가.

***** 취급액은 한 품목에 대해서 누적 판매액의 개념으로 사용이 되었으므로,
각 20분 마다의 취급액을 따로 계산하는 것이 좋을 것인지?

***** 보통은 한시간 단위로 판매가 묶인다. 그리고 이 제품을 20 20 20으로 파는 것과
30 30 으로 파는 것중 어느 것이 더 예상매출이 높은지 예측해야 한다.

-> 한 품목에 대해서 1시간 별로 총 취급액으로 묶은 데이터 프레임을 생성해야 한다.
    + 총 취급액이 아닌 20분간의 취급액으로 계산한 데이터 프레임을 생성해야 한다.

    -> 후자는 필요 없을지도? 그냥 노출(분)이 20인지 30인지 몇인지를 표시하는 컬럼만 추가해 보자

2019-01-01 6:00	 20 	100346	201072	테이트 남성 셀린니트3종	의류	 39,900 	 2,099,000
2019-01-01 6:00		    100346	201079	테이트 여성 셀린니트3종	의류	 39,900 	 4,371,000
2019-01-01 6:20	 20 	100346	201072	테이트 남성 셀린니트3종	의류	 39,900 	 3,262,000
2019-01-01 6:20		    100346	201079	테이트 여성 셀린니트3종	의류	 39,900 	 6,955,000
2019-01-01 6:40	 20 	100346	201072	테이트 남성 셀린니트3종	의류	 39,900 	 6,672,000
2019-01-01 6:40		    100346	201079	테이트 여성 셀린니트3종	의류	 39,900 	 9,337,000

와 같이 되어있는 데이터를 어떤식으로 융합할 것인가

2019-01-01 6:00	 20 	100346	201072	테이트 남성 셀린니트3종	의류	 39,900 	 6,672,000
2019-01-01 6:00	 20	    100346	201079	테이트 여성 셀린니트3종	의류	 39,900 	 9,337,000

의 형태로 변경하는 것이 좋을 것 같다.

//////////////////////////////////////////////////////////////////////////////////////////////////////////

-> 이상치 결측치의 처리가 가장 우선으로 해야한다.

무형을 제거 + 5만원으로 처리된 값들(해당시간에 품목에 대해서) 제거
방송시간이 10분 이하인 데이터는 수가 너무 적으므로 제거

data_over_10 = data_update_2[data_update_2['노출(분)'] >= 10].reset_index(drop=True)

//////////////////////////////////////////////////////////////////////////////////////////////////////////

***** 방송 분이 30이거나 40인 데이터들이 해당 방송의 마지막 데이터 값들이다.

목표는 20분 간격으로 한시간 동안 방송되거나 30분간격으로 방송되는 항목들에 대해서 중복된 값들을 통합해 주는 것이다.

-> 간혹 취급액이 줄어드는 경우도 있지만, 20분 간격으로 판매되는 항목은 40분의 값을, 30분간격으로 판매되는 값은 30분의 값을 취하였다.

모든 데이터가 00분에 판매를 시작한다는 가정하에.

    ->>>>> 체크 해봐야 할 것 같다.

따라서 pandas series이므로 logical_or을 사용하여 방송분이 30이거나 40인 값들에 대해서 새로 데이터 프레임을 형성한다.

그런다음에 이 값이 50000인 값들을 제해주면, 분석에 사용할 데이터 프레임이 완성되었다.

-> 중간값이 50000원인 항목은 이미 제거가 도었고, 마지막 항목이 50000으로 이상치인 값들만 제거해 주면 된다.

    ->>>>> 기존의 분이 30,40이면서 취급액이 50000원인 데이터의 갯수를 확인해보자.
    -> 396개의 데이터. 12641 - 12257 = 384개. -> 뭔가 덜 제거되었다.

-> 이상치 및 중복값들의 제거.

    data_last = data_over_10[np.logical_or(data_over_10['방송분']==30,data_over_10['방송분']==40)].reset_index(drop=True)

    data_last_2 = data_last[data_last['취급액']!=50000].reset_index(drop=True)

를 통해 data_last_2에 최종적으로 저장하였다.

    data_last_2 = data_last_2.iloc[:,1:]

으로 slicing을 통해 방송일시를 제거하였다.

//////////////////////////////////////////////////////////////////////////////////////////////////////////

***** 방송분이 30,40인 데이터들이 최종값이 아니다. -> 방송이 시작되는 분의 분포가 다음과 같다.

	    방송일시
방송분
0	    12430
5	    26      -> 방송분이 5분인 데이터는 다 노출시간이 15분짜리
10	    135
15	    54      -> 노출시간이 다 15분짜리
20	    11613
30	    1293
35	    26      -> 노출시간 15
40	    11348
45	    54      -> 노출시간 15
50	    374     -> 노출시간이 10, 15, 30인 데이터들이 존재한다.
               11월 이후에는 6:20에 시작하여 매 1시간별로 방송을 진행하였다. ex) 6:20 - 7:20

nov_start = pd.to_datetime('2019-11-01 06:20:00')
data_over_nov = data_update_1[data_update_1['방송일시']>=nov_start].reset_index(drop=True)

를 통해서 11월 이후의 데이터들에 대해서 방송일시를 timedelta를 통해 20분씩 앞당겨 주도록 하자. -> 큰 영향은 없을것으로 예상된다.

data_over_nov['방송일시'] = data_over_nov['방송일시'] - timedelta(minutes = 20)

그리고 방송분을 수정해 주었다.
data_over_nov['방송분'] = data_over_nov['방송일시'].dt.minute

+ 매주 토요일  18:00~18:20은 정보방송시간으로 추정 제외(당사사정에 따라 ±20 편성 변경 있음)
-> 일단은 최종적인 취급액에 대해서 분석을 시행할 것이므로 추후에 제거할 예정.

그런 후에 방송 분에 대해서 다시 확인해보자.

data_under_nov = data_update_1[data_update_1['방송일시']<nov_start] 를 통해 11월 이전의 데이터와
11월 이후의 수정된 데이터를 pd.concat을 통해 묶어준다.

data_mod = pd.concat([data_under_nov,data_over_nov]).reset_index(drop=True)

//////////////////////////////////////////////////////////////////////////////////////////////////////////

-> 이제 각 방송 분들에 대해서 분석해보자.

	방송일시	마더코드	상품코드	상품명	상품군	판매단가	취급액	방송일	방송시간	방송분
노출(분)
10.0	813
11.0	4
12.0	6
13.0	66
14.0	2
15.0	453
16.0	117
17.0	161
18.0	6
19.0	3
20.0	33057
22.0	4
23.0	9
25.0	11
26.0	12
27.0	2
30.0	2625
40.0	2

-> 노출(분)이 22-27인 값들은 30에서 단축방송된 항목들이므로 30으로 변경
15를 제외한 11-19의 값들은 20에서 단축. 따라서 20으로 변경
15분인 항목에 대해서 -> 원래 15분짜리 방송도 있지만, 20분이 단축된 것도 있다.

-> 처리해야함!
replace를 사용해 보자.
.replace([22,23,25,26,27],30)
.replace([11,12,13,14,16,17,18,19],20)
를 통해 범주화를 변경해 주었다.

//////////////////////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////////////////////

***** 중요

-> 해당데이터의 원래 노출(분)으로 수정해야 한다.

각자가 올바른 노출(분)으로 통일이 되어있는지 확인해보자.

data_mod.groupby('노출(분)').count()

            방송일시
노출(분)
10.0	    813
15.0	    453
20.0	    33422
30.0	    2663
40.0	    2       -> 이상치로 제거.

data_mod.groupby('방송분').count()

        방송일시
방송분
0.0	    1944
10.0	32
15.0	2
20.0	1807
30.0	163
40.0	1782
45.0	2
50.0	29

-> 데이터의 양이 적은것부터 확인해서 수정해보자.

노출(분)이 40인 데이터는 원래 40분인 데이터. -> 그 수가 너무 적으므로 이상치로 제외.
data_mod = data_mod[data_mod['노출(분)']!=40]

---------------------------------------------------------------------------------------------------

    노출(분)이 10인 데이터를 분석해 보자.


방송일시
방송분
0	137
10	138
20	138
30	136
40	137
50	127

-> 처음에 노출(분)이 10 이하인 데이터를 제거했는데 제거했으면 안됐다... -> 다시

//////////////////////////////////////////////////////////////////////////////////////////////////////////

다시 노출(분)이 10 이하인 데이터를 그대로 가져와서 다시 시작해보겠다.

노출(분)이 10 이하인 데이터들의 원래 방송시간을 찾아 원래 노출(분)으로 바꿔준다.

노출(분)이 2분짜리 방송은 원래 20분짜리. 수정해 주자
    -> data_mod.iloc[6677,1]=20

->>>>> 상품 코드별로 노출분이 2개 이상이라면 수정해야 할 항목으로 분류하자.

    -> 항상 작은 값이 큰값으로 들어가게 된다.

    apply를 활용한 방법이 뭐가 없을까? -> 각 group 별로 적용할수 있으면 좋을거 같다.

    grouped = data_mod.groupby('상품코드')를 통해 상품코드별로의 별개 그룹을 만들었다. 나중에 pd.concat으로 합쳐주면 될 것 같다.

->>>>> max_val_per_min = grouped.apply(lambda x : x['노출(분)'].max())

    각 상품코드 별로의 노출(분)최대값을 저장하였다.

->>>>> 상품 코드별로 노출(분)이 2개 이상인 데이터를 찾고 싶다.

    for key, group in grouped :
        if pd.Series.unique(group['노출(분)']).shape[0] >= 3 :
            print(key)
        else : pass

    -> 를 통해 노출(분) 이 3개 이상인 데이터들을 찾았다.

    -> 확인 결과, 해당 코드별로의 노출(분) 최대값으로 통일해주면 될 것 같다.

    -> 코드별로 나눈 grouped에 apply를 통해 값을 통일시켜 주자.

    for key, group in grouped :
        group['노출(분)'] = group['노출(분)'].max()

---------------------------------------------------------------------------------------------------------

groupby를 통해 나누어진 각 group들은 튜플로 구성이 되어있다. 첫번째[0]는 인덱스인 상품코드, 두번째[1]는 나머지 모두

for group in grouped :
    print(group[0])
    -> 상품코드 (groupby의 기준)

for group in grouped :
    print(group[1])
    -> 나머지 모든 컬럼과 값들(해당 상품코드의)

/////////////////////////////////////////////////////////////////////////////////////////////////////////

datetime.weekday() -> 0 월, 1 화, ... 6일

-> weekday()는 안되고, dayofweek는 된다. -> 왜인지는 모르겟음..

-> 11월 이후의 데이터들은 이미 timedelta를 통해 방송시간을 20분 앞당겨 주었으므로 별 손상이 없다.

문제는 11월 이전의 데이터들에 대해서 6시 이후에 방송 시간이 미뤄진 데이터들이 있다는 점이다.

//////////////////////////////////////////////////////////////////////////////////////////////////////////

3/9 일은 8시 이후로 20분씩 밀림
3/16
3/23
3/30
4/6

//////////////////////////////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////
데이터 전체를 2시간씩 당겨서 어느날에 방송한 것인지 컬럼을 추가

data_mod_2에다가.

data_mod_2['방송일'] = data_mod_2['방송일시'] - dt.timedelta(hours=2)
data_mod_2['방송일'] = data_mod_2['방송일'].dt.date

로 변경 완료.

이제 토요일 데이터프레임을 생성하고, 변경 준비

data_sat = data_mod_2[data_mod_2['방송일시'].dt.dayofweek == 5]
data_not_sat = data_mod_2[data_mod_2['방송일시'].dt.dayofweek != 5]

data_sat_under_11 = data_sat_2[data_sat_2['방송일시'] < '2019-11-01']

////////////////////////////////////////////////////////////
data_sat = data_mod_2[data_mod_2['방송일시'].dt.dayofweek == 5]
data_not_sat = data_mod_2[data_mod_2['방송일시'].dt.dayofweek != 5]
수정된 방송일이 아닌 방송일시를 사용하여 날짜가 올바르지 않게 들어갔다.
'방송일'로 다시 수정해야함

data_sat = data_mod_2[pd.to_datetime(data_mod_2['방송일']).dt.dayofweek == 5]
data_not_sat = data_mod_2[pd.to_datetime(data_mod_2['방송일']).dt.dayofweek != 5]
수정완료

data_mod_2[np.logical_and(data_mod_2['방송시간']==2,pd.to_datetime(data_mod_2['방송일']).dt.dayofweek==5)]
를 실행한 결과 아무값도 나오지 않음. 방송시간이 2를 넘어가는 토요일 방송은 존재하지 않는다.

그럼 이제 6시 이후로 방송시간이 20분 밀린 데이터들에 대해서 타임델타를 실행하여 20분을 앞당겨 준다.

grouped_sat_under_11.last().groupby('expose_min').count()
을 통해 11월 이전 토요일 데이터들의 마지막값들을 expose_min를 기준으로 groupby해주었다.

expose_min이 30인 데이터는 3-3일의 데이터로 0분, 30 분에 방송이 되어 따로 조정해줄 필요가 없다.
10인 데이터는 2개로 모두 수정해주면 된다.

***** groupby를 통해 생성된 grouped를 사용할 때,
    for 문 내에서 group만 호출하면 tuple 타입으로, key와 함께 호출해야 dataframe 타입으로서 정해진다.

/////////////////////////////////////////////////////////////////////////////////////////

-> 먼저 groupby를 요일별로 한다.
grouped_sat_under_11 = data_sat_under_11.groupby('방송일')

-> 그런뒤 grouped_sat_under_11에 대해서, 다시 상품코드 혹은 마더코드 별로의 groupby를 시행한다.
grouped_again = grouped_sat_under_11.groupby('마더코드')

-> 이 마더코드별로의 데이터프레임중 첫번째 값을 받아 0인지 아닌지를 확인한다.

-> 그리고 마더코드별로의 첫번째 값들의 방송분이 모두 0분이라면, 방송시간이 20분 밀린것이 아니므로,
시간에 대해서 변경해 줄 필요가 없다.
만약 마더코드별로의 첫번째 값들의 방송분중에 0이 아닌 값이 존재한다면, 방송시간이 20분 밀린것으로 판단,
그 날의 방송시간 18:00 이후로의 데이터들에 대해서 timedelta(min = 20)을 빼주어 시간을 조정해야 한다.

for key, group in grouped_again :

    를 통해 key들을 뽑아내야 한다. -> 시간이 밀린 날짜가 언제인지.

    -> group 또한 데이터 프레임의 형태이다. -> 이를 활용해야함.

    각 그룹은 날짜를 통해 분류된 grouped_sat_under_11( 11월 이전의 토요일 데이터들(18:00-02:00)

    컬럼을 새로 추가하는 시도?

    각 요일별, 상품코드 별로 몇뻔째에 방송했는지 인덱스를 통해 순위를 매겨보는 것은 어떠한가?

    -> data_mod_2.groupby(['방송일','상품코드']).apply()

    rank(axis=0, method='min')

    -> 랭크를 통한 컬럼의 추가

    data_mod_2['rank'] = data_mod_2.groupby(['방송일','상품코드'])['방송일시'].rank(axis='index', method='min')

    -> 를 통해 방송일, 상품 코드로 그룹화를 시킨후, 인덱스 순으로(방송일시) rank를 매겨 컬럼을 추가하였다.

    -> 토요일 데이터프레임을 다시 설정해 주자.

data_sat = data_mod_2[pd.to_datetime(data_mod_2['방송일']).dt.dayofweek == 5]
data_not_sat = data_mod_2[pd.to_datetime(data_mod_2['방송일']).dt.dayofweek != 5]

data_sat_2 = data_sat[np.logical_or(data_sat['방송시간']>=18,data_sat['방송시간']<=2)]

data_sat_under_11 = data_sat_2[data_sat_2['방송일시'] < '2019-11-01']
data_sat_over_11 = data_sat_2[data_sat_2['방송일시']>='2019-11-01']

grouped_sat_over_11 = data_sat_over_11.groupby('방송일')
grouped_sat_under_11 = data_sat_under_11.groupby('방송일')

-> rank가 4 이상인 값이 생기게 되었다. -> 3

-> 방송일에 대해서도 -> 처리하기 까다롭다
data_sat_에 대해서만 rank를 매긴뒤에 해보는 것은 어떨까?

다시 data_mod_2에서 마지막 컬럼을 제거한 뒤에 data_sat을 변경해보자.

for key, group in grouped_sat_under_11:
    if pd.to_datetime('18:00:00') not in group['방송일시'].dt.time:
        print(key)
    else : pass

    를 통해 모든 토요일 데이터 내에는 18:00 데이터가 존재 하지 않는 다는 것을 확인 하였다.

    -> 안됨.

/////////////////////////////////////////////////////////////////////////////

for key, group in grouped_sat_under_11:
    if True in pd.Series.tolist(group[group['rank']==1]['방송분'] != 0) :
        print(key)

    -> 성공적으로 해당 날짜를 추출하는데에 성공함.
    -> 빈 리스트를 만든 뒤에 append를 통해 추가해 주자.
    -> 굳이 numpy array로 만들어야 하나? 그냥 일반 리스트여도 무방할 것 같다.

-> 이제 해당 날짜의 데이터들의 시간을 20분씩 앞당기면 된다.

18:20 부터 02:00의 데이터를 timedelta를 통해 변경해주자.

    pd.where을 사용하면 될 것 같다.

    -> 조건 : 해당 날짜의 18:00부터 02:00의 데이터들에 대해서, 방송일시 -timedelta(minute=20)

    data_mod_2['방송일시'].where(np.logical_and(data_mod_2['방송일시'].dt.date date_need_delay, data_mod_2['방송시간']),)

    만약 조건절에 시간이 2보다 작고 18보다 크다는 조건이 존재한다면 이 조건이 두개의 조건을 사용하므로,
    조건을 2개로 최소화 하기 위해, 방송시간 컬럼을 사용해보자. -> 방송시간이 이 값들 내에 존재한다면,

    data_mod_2['방송시간'] in time_range
    time_range = [0, 1, 2, 18, 19, 20, 21, 22, 23]
    로 시도해보자.

        data_mod_2['방송일시'].where(np.logical_and(data_mod_2['방송일시'].dt.date in date_need_delay, data_mod_2['방송시간'] in time_range)
        , data_mod_2['방송일시'] - timedelta(minute=20))

    ->  The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
    와 같은 오류가 발생하였다.

    .item()을 사용하면 될까?

    -> in이 아니라 isin()을 사용하니까 해결되었다.

    -> cond1 = data_mod_2['방송일시'].dt.date.isin(date_need_delay) 으로 조건 1을 만들어 놓았다.
    cond2 = data_mod_2['방송시간'].isin(time_range)

data_mod_2['방송일시'].where(np.logical_and(cond1, cond2),data_mod_2['방송일시'] - timedelta(minutes=20))

    -> 이제 변경되었는지 확인해보자.

    -> 제대로 안된것 같다... 다시 해야할듯...

    cond3 = data_mod_2['방송시간'] == 18 을 통해서 토요일 자료들중 18시에 방송한것들은 모두 20분 당기도록.
    그중에 19:00데이터가 밀렸으면 20분 당기고, 아니면 그대로 두는 형식으로.
    2월달까지는 안밀렸고, 3월달 부터는 10월까지는 밀렸다.

    time_range = [0, 1, 2, 19, 20, 21, 22, 23] 으로 time_range를 변경하였다.

    -> np.where(배열에 대한 조건문, 참일때 값, 거짓일때 값) 반대로 넣었는가 값을?

    cond1과 cond2 두 조건을 만족하는 일시에 대해서 timedelta로 20분을 줄여주어야한다.
    data_mod_2['방송일시'] = np.where(np.logical_and(,), 만족할때, 만족하지 못할때)
                            1번째 - cond1과 cond2를 만족.
                            날짜에 포함되고, 해당 시간이라면 timedelta(minutes=20)을 사용.

                            2번째 - cond1을 안만족하는 토요일. 1월 2월의 토요일과 cond3
                            18시의 데이터만 timedelta(minutes=20)을 적용

    -> '방송일시'가 date_need_delay에 있는 것이 아니라 '방송일일'이 date_need_delay에 존재해야 한다
    cond1 = pd.to_datetime(data_mod_2['방송일']).isin(date_need_delay)

    1) 3월부터 10월까지 19:00 - 02:00의 데이터를 변경하는 것은 cond1과 cond2를 사용하면 된다.

        data_mod_2[np.logical_and(cond1, cond2)]['방송일시'] -timedelta(minutes=20) 을 통해 확인해 보았다.

    2) 1- 10월까지 18:00 - 18:40의 데이터를 20분 앞당기는 것을 해결해야함.

        -> 날짜 추출방법 :
            data_sat_under_11에서 방송일을 중복제거한뒤 리스트에 추가하면 된다.
            date_sat = []
            for key, group in grouped_sat_under_11:
                 date_sat.append(key)
            을 통해 추가해 놓았다.

            cond4 = pd.to_datetime(data_mod_2['방송일']).isin(date_sat)
            cond3 = data_mod_2['방송시간'] == 18

            data_mod_2[np.logical_and(cond3, cond4)] 을 통해 확인해 보았다.

    이제 실질적으로 시간을 앞당겨 보자.

    data_mod_2['방송일시'] = np.where(np.logical_and(cond1,cond2), data_mod_2['방송일시'] - timedelta(minutes=20), data_mod_2['방송일시'])
    data_mod_2['방송일시'] = np.where(np.logical_and(cond3,cond4), data_mod_2['방송일시'] - timedelta(minutes=20), data_mod_2['방송일시'])

    ///////
    확인을 해보자

    data_mod_2['방송시간'] = data_mod_2['방송일시'].dt.hour
    data_mod_2['방송분'] = data_mod_2['방송일시'].dt.minute

    data_sat = data_mod_2[pd.to_datetime(data_mod_2['방송일']).dt.dayofweek == 5]
    data_not_sat = data_mod_2[pd.to_datetime(data_mod_2['방송일']).dt.dayofweek != 5]

    data_sat_2 = data_sat[np.logical_or(data_sat['방송시간']>=18,data_sat['방송시간']<=2)]

    data_sat_under_11 = data_sat_2[data_sat_2['방송일시'] < '2019-11-01']
    data_sat_over_11 = data_sat_2[data_sat_2['방송일시']>='2019-11-01']

    grouped_sat_over_11 = data_sat_over_11.groupby('방송일')
    grouped_sat_under_11 = data_sat_under_11.groupby('방송일')

    data_sat_under_11['rank'] = data_sat_under_11.groupby(['방송일','상품코드'])['방송일시'].rank(axis='index', method='min')

    -> 성공적으로 시간 변경을 해 주었다!

////////////////////////////////////////////////////////////////////////////////////////

이제 각 시간의 상품코드별로 마지막 값들만으로 새로운 데이터 프레임을 생성해보자.

groupby를 각 시간, 상품코드 별로 묶어야 한다.

data_mod_3 = data_mod_2.groupby(['방송일','상품코드']).tail(n=1).reset_index(drop=True)
를 통해 인덱스를 재 설정하면서, 마지막값들을 받아왔다.

-> 결국 관심있는 것은 해당 시간에서의 취급액.

여기서 취급액이 50000인 값들을 제거해준다.

data_mod_4 = data_mod_3[data_mod_3['취급액']!=50000].reset_index(drop=True)을 통해 인덱스를 드랍하였다.

//////////////////////////////////////////////////////////////////////////////////////

분석을 위한 데이터 프레임의 전처리가 어느정도 완성 되었다.

이제 데이터들을 살펴 보도록 하자!

data_mod_5 = data_mod_4.loc[:,['마더코드','상품코드','상품명','상품군','판매단가','취급액','방송일','방송시간','expose_min']]
을 사용하자. -> '방송분'은 크게 관심이 없는 컬럼이므로 제거하였다.
            -> '방송일시'도 딱히 필요 없을 것 같다.

마더코드 별로 상품코드가 몇종류씩 존재하는지 확인해보고 싶다.

grouped_2 = data_mod_5.groupby('마더코드')
for key, group in grouped_2 :
    print(group['상품코드'].value_counts())

data_mod_5['마더코드'].value_counts()   -> 각 마더코드 별로 몇개의 데이터가 존재하는지 확인.
data_mod_5['마더코드'].value_counts().shape[0]  -> 총 몇종류의 마더코드가 존재하는지.

-> 총 687종의 마더코드가 존재한다.

for key, group in grouped_2:
    if group['상품코드'].value_counts().shape[0] >= 10 :
        print(key)

-> 마더코드 별로 그 내의 상품코드의 종류가 10가지 이상인 마더코드들을 확인해 봤다.

code_over_10 = []
for key, group in grouped_2:
    if group['상품코드'].value_counts().shape[0] >= 10 :
        code_over_10.append(key)
len(code_over_10)

-> 33개의 마더코드가 10개 이상의 상품코드를 가지고 있었다.

data_mod_4를 사용하여 데이터를 살펴보자.

seaborn을 통한 데이터 시각화

data_mod_5의 취급액 분포를 확인해 보자.

    data_mod_5['취급액'].describe()

        count    1.230600e+04
        mean     3.115657e+07
        std      2.410639e+07
        min      1.030000e+05
        25%      1.257500e+07
        50%      2.546150e+07
        75%      4.457425e+07
        max      3.220090e+08

        -> 최소값과 최대값 사이의 차이가 너무 크다.

    -> 1.5e8보다 취급액이 큰 데이터를 살펴보니
    data_mod_5[data_mod_5['취급액']>1.5e8].count()

        22개의 데이터가 발견되었다.

    -> 분포를 살펴보자.
    sns.distplot(a = data_mod_5['취급액'], hist = True, kde = False, rug = True)















