데이터의 이해

	데이터의 이해

		데이터의 유형
			정성적 데이터 - 저장, 검색, 분석에 많은 비용이 소모되는언어, 문자 형태의 데이터 EX) 회사의 매출이 증가함
			정형적 데이터 - 정형화된 데이터로 수치, 도형, 기호 등의 형태를 가진 데이터 EX) 나이, 몸무게, 주가

		지식경영의 핵심 이슈
			암묵지 - 학습과 경험을 통해 개인에게 체화되어 있지만 겉으로 드러나지 않는 지식. 사회적으로 중요하지만 공유되기 어려움. 공통화, 내면화
			형식지 - 문서나 매뉴얼처럼 형상화된 지식. 전달과 공유가 용이함. 표출화, 연결화

		DIKW 피라미드
			DATA - 존재형식을 불문, 타 데이터와 상관관계가 없는 가공전의 순수한 수치나 기호.
			INFORMATION - 데이터의 가공 및 상관관계간 이해를 통해 패턴을 인식하고 그 의미를 부여한 데이터.
			KNOWLEDGE - 상호 연결된 정보 패턴을 이해하여 이를 토대로 예측한 결과물
			WISDOM - 근본 원리에 대한 깊은 이해를 바탕으로 도출되는 창의적인 아이디어

		데이터베이스의 정의
			EU - 전자식 또는 기타 수단. 개별적으로 독립된 저작물, 데이터 또는 기타 소재의 소집물
			국내 저작권법 - 소재를 체계적으로 배열 또는 구성한 편집물. 개별적으로 접근하거나 그 소재를 검색할 수 있도록 한 것.
			국내 컴퓨터용어사전 - 동시에 복수의 적용 업무를 지원할 수 있도록 복수 이용자의 요구에 대응해 데이터를 받아들이고 저장, 공급하기 위하여 일정한 구조에 따라 편성된 데이터의 					집합

		데이터베이스 특징
			통합된 데이터 - 동일한 내용의 데이터가 중복되어있지 않다. 중복은 관리상 복잡한 부작용을 초래
			저장된 데이터 - 자기 디스크나 자기 테이프 등과 같이 컴퓨터가 접근할 수 있는 저장매체에 저장. 기본적으로 컴퓨터 기술을 바탕으로 한 것
			공용 데이터 - 여러 사용자가 서로 다른 목적으로 공동으로 이용. 대용량화 되고 구조가 복잡함
			변화되는 데이터 - 저장된 내용은 현 시점에서의 상태를 나타냄. 새로운 데이터의 삽입과 기존 데이터의 삭제, 갱신으로 항상 변화하면서 정확한 데이터를 유지해야 함

		빅데이터의 정의
			관점에 따른 정의
				규모에 중점 - 일반적인 데이터베이스 소프트웨어로 저장, 관리, 분석할 수 있는 범위를 초과하는 규모의 데이터
				분석 비용 및 기술에 초점 - 다양한 종류의 대규모 데이터로부터 저렴한 비용의 가치를 추출, 데이터의 초고속 수집, 발굴, 분석을 지원하도록 고안된 아키텍처
				3V - VOLUME, VARIETY, VELOCITY
			정의의 범주 및 변화
				데이터의 변화 - 3V
				기술의 변화 - 데이터 처리, 저장, 분석기술 및 아키텍처. 클라우드 컴퓨팅 활용
				인재, 조직의 변화 - DATA SCIENTIST와 같은 새로운 인재. 데이터 중심 조직

		출현의 배경과 변화
			산업계 - 고객 데이터의 축척, 보유를 통해 데이터에 숨어있는 가치 발굴
			학계 - 거대 데이터를 다루는 학문 분야의 증가. 기술아키텍처 및 통계 도구의 발전
			기술발전 - 관련기술(저장 기술, 인터넷 보급, 클라우드 컴퓨팅, 모바일 혁명)의 발달

		빅데이터에 대한 기대의 비유적 표현
			산업혁명의 석탄과 철 - 제조업 뿐 아니라 서비스 분야의 생산성을 획기적으로 끌어올려 사회/경제/문화/생활 전반에 혁명적 변화
			21세기의 원유 - 경제 성장에 필요한 정보를 제공함으로써 산업 전반의 생산성을 한 단계 향상시키고, 기존에 없던 새로운 범주
			렌즈 - 생물학에 영향
			플랫폼 - 공동 활용의 목적으로 구축된 유무형의 구조물

		빅데이터가 만들어내는 본질적인 변화
			사전처리 -> 사후처리
			표본조사 -> 전수조사
			질 -> 양
			인과관계 -> 상관관계

	데이터의 가치와 미래

		빅데이터의 가치 산정이 어려운 이유
			데이터 활용 방식
			새로운 가치 창출
			분석기술 발전

		빅데이터의 영향력
			기업 - 혁신, 경쟁력제고, 생산성 향상 -> 빅데이터를 활용해 소비자의 행동을 분석하고 시장 변동을 예측해 비즈니스 모델을 혁신하거나 신사업을 발굴한다.
			정부 - 환경 탐색, 상황분석, 미래대응 -> 가상, 인구이동, 각종통계, 법제 데이터 등을 수집해 사회변화를 추정, 정보를 추출한다.
			개인 - 목적에 따른 활용 -> 빅데이터를 서비스하는 기업의 출현으로 비용이 지속적으로 하락, 인지도 향상의 측면에서 빅데이터를 활용

			-> 생활 전반의 스마트화

		비즈니스 모델

			빅데이터의 활용사례
				관점에 따른 정의
					구글 - 사용자의 로그 데이터를 활용한 검색엔진 개발
					월마트 - 고객의 구매패턴을 분석해 상품 진열에 활용

				정부 - 실시간 교통정부 수집, 기후 정보, 소방 서비스 등을 위해 실시간 모니터링 실시하여 국가 안전 확보에 활용
				개인 -  정치인과 가수. 사회관계망 분석을 활용해 유세 지역 선거 및 팬들의 음악 청취 기록을 분석해 공연시 노래 순서 선정
			
			빅데이터 활용 기본 테크닉
				연관 규칙 학습, 군집분석, 유전 알고리즘, 기계학습, 회귀분석, 감정분석, 소셜네트워크분석=사회관계망분석

		위기요인과 통제 방안

			사생활 침해 -> 동의에서 책임으로
			책임 원칙 훼손 -> 결과 기반 책임 원칙 고수
			데이터 오용 -> 알고리즘 접근 허용

		미래의 빅데이터

			데이터 - 모든 것의 데이터화
			기술 - 진화하는 알고리즘, 인공지능
			인력 - 데이터 사이언티스트, 알고리즈미스트


	가치 창조를 위한 데이터 사이언스와 전략 인사이트

		빅데이터 회의론의 원인

			부정적 학습효과 - 과거의 고객관리관계 : 공포 마케팅, 투자대비 효과 미흡
			부적정한 성공사례 - 빅데이터가 필요없는 분석사례, 기존 CRM의 분석 성과를 빅데이터 분석 성과로 과대포장
			
			-> 분석을 통해 가치를 만드는 것에 집중

		일차원적인 분석 애플리케이션

			금융 서비스 - 신용점수 산정, 사기 탐지, 가격 책정
			병원 - 가격 책정, 고객 로열티, 수익 관리
			에너지 - 트레이딩, 공급, 수요 예측
			정부 - 사기 탐지, 사례관리, 범죄 방지

		데이터 사이언스

			데이터 사이언스의 의미 - 데이터 공학, 수학, 통계학, 컴퓨터공학, 시각화, 해당 분야의 전문 지식을 종합한 학문

			데이터 사이언스의 구성요소 - ANALYTICS(분석적 영역), 비즈니스 분석(비즈니스 컨설팅 영역), IT(데이터 처리와 관련된 IT영역)
				HARD SKILL - 빅데이터에 대한 이론적인 지식. 분석 기술에 대한 숙련
				SOFT SKILL - 통찰력 있는 분석, 설득력 있는 전달, 다분야간 협력
			
				-> 단순 통계 및 데이터 처리 능력보다 스토리텔링, 커뮤니케이션, 창의력, 열정, 직관력 등의 인문학적 요소가 필요함

데이터 분석 기획

	데이터 분석 기획의 이해

		분석 기획 방향성 도출

			분석 기획의 특징 - 실제 분석을 수행하기에 앞서 분석을 수행할 과제를 정의하고, 의도했던 결과를 도출할 수 있도록 적절하게 관리할 수 있는 방안을 사전에 계획하는 작업
				데이터 사이언티스트의 역량 - 수학/통계학적 지식, 정보기술, 비즈니스에 대한 이해와 전문성

			분석 대상과 방법 - 분석 대상과 분석 방법에 따라서 네가지로 나뉜다.
				OPTIMIZATION	- 대상을 알고 분석 방법도 아는 경우
				INSIGHT		- 대상을 모르고 분석 방법은 아는 경우
				SOLUTION	- 대상을 알고 분석 방법은 모르는 경우
				DISCOVERY	- 대상을 모르고 분석 방법도 모르는 경우

			목표 시점별 분석기획 방안  - 당면한 분석 주제의 해결(과제 단위), 지속적 분석 문화 내재화(마스터 플랜 단위)

			분석 기획시 고려사항
				가용 데이터에 대한 고려
				분석을 통해 가치가 창출될 수 있는 적절한 활용방안과 유즈케이스 탐색이 필요
				분석 수행시 발생하는 장애요소들에 대한 사전계획 수립이 필요

		분석 방법론 개요

			기업의 합리적인 의사결정을 가로막는 장애요소 - 고정관념, 편향된 생각, 프레이밍 효과
			방법론의 적용 업무의 특성에 따른 모델 - 폭포수 모델, 프로토타입 모델, 나선형 모델

		KDD 분석 방법론

			데이터셋 선택
			데이터 전처리
			데이터 변환
			데이터 마이닝
			결과 평가

		CRISP-DM 분석 방법론

			업무 이해
			데이터 이해
			데이터 준비
			모델링
			평가
			전개
		
		빅데이터 분석 방법론

			빅데이터 분석의 계층적 프로세스
				단계		↑PROCESS GROUP
				테스크		MAPPING
				스텝		↓UNIT PROCESSING

			빅데이터 분석 방법론의 5단계
				분석 기획 - 비지니스 이해 및 범위 설정, 프로젝트 정의 및 계획 수립, 프로젝트 위험계획 수립
				데이터 준비 - 필요 데이터 정의, 데이터 스토어 설계, 데이터 수집 및 정합성 점검
				데이터 분석 - 분석용 데이터 준비, 텍스트 분석, 탐색적 분석, 모델링, 모델 평가 및 검증, 모델 적용 및 운영방안 수립
				시스템 구현 - 설계 및 구현, 시스템 테스트 및 운영
				평가 및 전개 - 모델 발전 계획 수립, 프로젝트 평가 및 보고
			
		분석과제 발굴 방법론

			하향식 접근 방법 - 분석과제가 주어지고 이에 대한 해법을 찾기 위하여 각 과정이 체계적으로 단계화되어 수행하는 방식
				문제적 탐색 - 비즈니스 모델기반 문제탐색(업무, 제품, 고객, 규제와 감사, 자원 인프라) 외부사례 기반 문제탐색(벤치마킹)
				문제정의 - 비즈니스 문제를 데이터의 문제로 변환하여 정의
				해결방안탐색 - 분석역량, 분석기법 및 시스템 으로 해결 방안 탐색 
				타당성 검토 - 경제적 타당성, 데이터 및 기술적 타장성 검토(분석역량)

			상향식 접근 방법 - 문제의 정의 자체가 어려운 경우 데이터를 기반으로 문제를 지속적으로 개선하는 방식. 
				정의 - 기업이 보유하고 있는 다양한 원천 데이터로부터 분석을 통하여 통찰력과 지식을 얻는 접근방법

				특징
					하향식 접근법은 논리적 단계별 접근법으로 최근 복잡하고 다양한 환경에서 발생하는 문제를 해결하기 어렵기 때문에 디자인적인 사고 접근법을 통해
					WHY->WHAT 관점으로 존재하는 데이터 그 자체를 객관적으로 관찰하여 문제를 해결하려는 접근법을 사용
					상향식 접근법은 비지도 학습 방법으로 수행되며, 데이터 자체의 결합, 연관성, 유사성을 중심으로 접근
					시행착오를 통한 문제 해결(프로토타이핑 접근법)
			
			-> 분석과제 정의서를 통해 분석별 필요한 소스데이터, 분석 방법, 데이터 입수 및 분석의 난이도, 분석 수행주기, 검증 오너십, 상세 분석 과정 등을 정의

		분석과제 관리를 위한 5가지 주요 영역

			DATA COMPLEXITY, SPEED, DATA SIZE, ANALYTIC COMPLEXITY, ACCURACY&PRECISION

		분석 프로젝트의 특성

			분석가 목표 - 개별적인 분석업무 수행 뿐만 아니라 전반적인 프로젝트 관리 또한 중요

			분석가의 입장 - 데이터 영역과 비즈니스 영역의 현활을 이해하고, 프로젝트의 목표인 분석의 정확도 달성과 결과에 대한 가치 이해를 전달하는 조정자로서의 분석가 역할이 중요

	분석 마스터 플랜

		마스터 플랜 수립 프레임 워크

			우선순위 고려요소 - 전략적 중요도, 비즈니스 성과/ROI, 실행 용이성 -> 적용 우선순위 설정
			적용범위/방식 고려요소 - 업무 내재화 수준, 분석 데이터 적용 수준, 기술 적용 수준 -> ANALYTICS 구현 로드맵 수립

		ROI 관점에서 빅데이터의 핵심 특징 (우선순위 평가에 활용하기 위해)

			3V (투자비용 요소) + VALUE (비즈니스 효과) = 4V

		분석 거버넌스 체계 구성요소

			PROCESS - 과제 기획 및 운영 프로세스
			SYSTEM - 분석관련시스템
			DATA - 데이터
			HUMAN RESOURCE - 분석교육/마인드 육성체계
			ORGANIZATION - 분석기획 및 관리 수행 조직

		데이터 분석 수준진단

			분석 준비도 - 분석업무, 분석인력/조직, 분석기법, 분석데이터, 분석문화, 분석인프라

			분석 성숙도 - CMMI(CAPABILITY MATURITY MODEL INTERGRATION)을 통해 평가. 도입->활용->확산->최적화 (비즈니스, 조직 및 역량, IT)
				-> 분석 수준 진단 결과 - 정착형, 확산형, 준비형, 도입형

		데이터 거버넌스 체계 수립

			데이터 거버넌스 - 전사 차원의 모든 데이터에 대하여 정책 및 지침, 표준화, 운영조직 및 책임 등의 표준화 된 관리 체게를 수립하고 운영을 위한 프레임워크 및 저장고를 구축하는 것
					마스터 데이터, 메타 데이터, 데이터 사전은 데이터 거버넌스의 중요한 관리 대상

			데이터 거버넌스의 구성요소 - 원칙, 조직, 프로세스

			데이터 거버넌스 체계 - 데이터 표준화, 데이터 관리 체계, 데이터 저장소 관리, 표준화 활동 

		데이터 분석을 위한 3가지 조직구조

			집중구조, 기능구조, 분산구조

		분석과제 관리 프로세스

			과제 발굴
				분석 IDEA 발굴 -> 분석과제 후보제안 (과제 후보 POOL) -> 분석과제 확정 (전사분석조직 -> 분석과제를 과제제안자, 과제추진팀으로)

			과제 수행
				팀 구성 -> 분석과제 실행 (전사분석조직이 과제수행 지원) -> 분석과제 진행관리 (전사분석조직) -> 결과 공유/개선 (과제 결과 POOL)

데이터 분석

	데이터 분석 개요

		데이터 분석 기법의 이해

			데이터 처리 과정 - 데이터 분석을 위해 데이터웨어하우스(DW)나 데이터마트(DM)을 통해 분석데이터를 구성
					신규 데이터나 DW에 없는 데이터는 기존운영시스템에서 직접 가져오거나 운영데이터저장소에서 정제된 데이터를 가져와서 DW의 데이터와 결합하여 활용

			시각화 기법 - 가장 낮은 수준의 분석이지만 잘 사용하면 복잡한 분석보다 더 효율적이며, 대용량 데이터를 다룰 때와 탐색적 분석을 할 때 시각화는 필수

			공간분석 - 공간적 차원과 관련된 속성들을 시각화하는 분석으로 지도 위에 관련된 속성들을 생성하고 크기모양, 선 굵기 등을 구분하여 인사이트를 얻음

			탐색적 자료분석(EDA) - 다양한 차원과 값을 조합해 가며 특이점이나 의미있는 사실을 도출하고 분석의 최종 목적을 달성해 가는 과정
				-> 4가지 주제 : 저항성의 강조, 잔차 계산, 자료변수의 재표현, 그래프를 통한 현시성

			통계분석 - 어떤 현상을 종합적으로 한눈에 알아보기 쉽게 일정한 체계에 따라 숫자와 표, 그림의 형태로 나타내는 것

			데이터 마이닝 - 대용량의 자료로부터 정보를 요약하고 미래에 대한 예측을 목표로 자료에 존재하는 관계, 패턴, 규칙 들을 탐색하고 이를 모형화함으로써 이전에 알지 못한 유용한 지					식을 추출하는 분석 방법
				-> 방법론 : 기계학습(인공신경망, 의사결정나무, 클러스터링, SVM), 패턴인식(연관규칙, 장바구니분석)

	R프로그래밍 기초

		R의 특징

			오픈소스 프로그래밍으로 통계, 데이터마이닝과 그래프를 위한 언어
			다양한 최신 통계분석과 마이닝 기능을 제공, 패키지가 수시로 업데이트 됨
			다른 통계분석 도구와 비교하여 오픈소스이며, 최근 알고리즘 기술반영이 빠르다.

			특징
				오픈소스 프로그램
				뛰어난 그래픽 및 성능
				시스템 데이터 저장 방식
				모든 운영체게에서 사용 가능
				표준 플랫폼(S 언어 기반)
				객체 지향언어이면서 함수 언어

		패키지 

			INSTALL.PACKAGES("패키지명")
			LIBRARY(패키지명)

		변수 다루기

			R에서는  변수명만 선언하고 값을 할당하면 자료형태를 스스로 인식하고 선언함
			화면에 값을 출력할 때, PRINT()를 사용하지 않고 변수 값만 표현해도 내용을 출력함
			대입 연산자는 <-를 사용하는 것을 추천
			메모리에 불필요한 변수가 있는지 확인시에는 LS(), 제거할때는 RM()를 사용

		통계량 계산

			MEAN() - 평균
			MEDIAN() - 중간값
			SD() - 표준편차
			VAR() - 분산
			COV() - 공분산
			COR() - 상관계수

		함수의 생성

			R은 함수형 언어로 프로그래머가 직접 활용 가능한 함수를 활용할 수 있음
			FUNCTION(매개변수1, 매개변수2, ...)로 선언
			표현식은 변수 할당, 조건문(IF ELSE)과 반복문(FOR, WHILE, REPEAT) 그리고 전달값으로 구성

		연산자

			$ - 요소 뽑아내기
			%(연산자)% - 특수연산자

		데이터 입력과 출력

			R에서는 텍스트 뿐만 아니라 데이터베이스와 다양한 통계프로그램에서 작성된 데이터를 불러들여서 적절한 데이터 분석을 수행할 수 있음
			부동소수점 표현시 7자리로 기본 셋팅. OPTION()함수, DIGIT="숫자"를 지정해서 자릿수 변경 가능
			문자열을 파일로 저장시 : CAT("저장할 문자열", FILE="파일명')
			역슬래쉬를 인식하지 못하므로 슬래쉬나 이중역슬래쉬로 파일의 경로를 지정

			외부파일  읽기 - READ.CSV, READ.TABLE("파일명", SEP="구분자")

		데이터 구조와 데이터 프레임

			데이터 구조의 정의
				벡터와 리스트 데이터프레임 모두 원소를 위치로 인덱싱 가능, 인덱싱으로 여러 개 원소로 구성된 하위 데이터 생성 가능, 원소들에 이름 부여 가능
				원소 자료형 -> 벡터는 동질적, 리스트와 데이터프레임은 이질적

				단일값 - 원소가 하나인 벡터로 인식/처리
				행렬 - 원소가 하나인 벡터로 인식/처리
				배열 - 3원소가 하나인 벡터로 인식/처리
				요인 - 고유값이 요인의 수준으로 구성된 벡터(범주형 변수, 집단 분류)

			리스트 다루기 
				리스트의 원소 선택 - L[[N]], L[["NAME"]], L$NAME

			행렬 다루기
				행렬 설정 - DIM(VEC) <- C(2,3)
				행과 열 이름 설정 - ROWNAME(MTRX) <- C("ROWNAME1","ROWNAME2",...)
			
			데이터 구조 변환
				AS.변환후의데이터타입(변환할데이터) EX) AS.LIST(VEC), AS.DATA.FREME(MAT)
				리스트를 벡터로 변환 - UNLIST(LIST)

			집단으로 분할하기
				벡터 - SPLIT(VEC, FAC) 벡터값과 팩터값의 길이가 같아야함
				데이터프레임 - SPLIT(DFM, FAC)

			함수 적용하기
				벡터, 행렬 - APPLY(MTR,1,FUNC) 행단위 함수적용, APPLY(MTR,2,FUNC) 열단위 함수적용
				리스트 - LAPPLY(LST,FUNC), SAPPLY(LST,FUNC)
				데이터프레임 - LAPPLY(DFM, FUNC), SAPPLY(DFM, FUNC), APPLY(DFM, FUNC)

			집단별로 함수 적용하기
				TAPPLY(VEC,FAC,FUNC), BY(DFM, FAC, FUNC)
			
			병렬 벡터들과 리스트들에 함수 적용하기
				벡터 - MAPPLY(FUNC, VEC1, VEC2, ...)
				리스트 - MAPPLY(FUNC, LST1, LST2, ...)

			문자열 다루기
				문자열의 길이 - NCHAR("문자열")
				벡터의 길이 - LENGTH(VEC)
				문자열 연결하기 - PASTE("단어", "문장", SCALAR)
				하위 문자열 추출하기 - SUBSTR("문자열", 시작번호, 끝번호)
				구분자로 문자열 추출하기 -STRSPLI("문자열", 구분자)
				문자열 대체하기 - SUB("대상문자열","변경문자열",S) 

			날짜 다루기
				문자열 -> 날짜 - AS.DATE("2014-12-25")
						AS.DATE("12/25/2014",FORMAT="%M/%D/%Y")
				날짜 -> 문자열 - FORMAT(SYS.DATE(), FORMAT="%M/%D/%Y")

				FORMAT 인자값
					%b - 축약 월 이름 "jan"
					%B - 전체 월 이름 "january"
					%d - 두자리 일
					%m - 두자리 월
					%y - 두자리 년 "14"
					%Y - 네자리 년 "2014"

	데이터 마트

		데이터 마트

			데이터 웨어하우스와 사용자 사이의 중간층에 위치한 것으로, 하나의 주제 또는 하나의 부서 중심의 데이터 웨어하우스

		요약변수와 파생변수

			요약변수 - 수집된 정보를 분석에 맞게 종합한 변수로 데이터 마트에서 가장 기본적인 변수. 많은 모델이 공통으로 사용할 수 있어 재활용성이 높음
			파행변수 - 사용자가 특정 조건을 만족하거나 특정 함수에 의해 값을 만들어 의미를 부여한 변수. 매우 주관적일 수 있으므로 논리적 타당성을 갖출 필요가 있음

		데이터 마트에서 사용되는 주요 패키지

			RESHAPE 패키지 - 변수를 조합해 변수명을 만들고 변수들을 시간, 상품 등의 차원에 결합. 다양한 요약변수와 파생변수를 쉽게 생성하여 데이터 마트를 구성할 수 있게 해주는 패키지
				MELT() - 쉬운 CASTING을 위해 데이터를 적당한 형태로 만들어주는 함수
				CAST() - 데이터를 원하는 형태로 계산 또는 변형시켜주는 함수

			SQLDF 패키지 - R에서 SQL명령어를 사용가능하게 해주는 패키지.
				HEAD([DF]) -> SQLDF("SELECT * FROM [DF] LIMIT 6")
				SUBSET([DF],[COL] %IN% C("BF","HF")) -> SQLDF("SELECT * FROM [DF] WHERE [COL] IN ('BF','HF')")
				MERGE([DF1],[DF2]) -> SQLDF("SELECT * FROM [DF1],[DF2]")

			PLYR 패키지 - APPLY 함수를 기반으로 데이터와 출력변수를 동시에 배열로 치환하여 처리하는 패키지
				SPLIT - APPLY - COMBINE 방식으로 데이터를 분리하고 처리한 다음, 다시 결합하는 등 필수적인 데이터 처리 기능 제공
				분리전 데이터 타입의 첫글자 + 재결합 후 데이터 타입의 첫글자 + PLY 없으면 _
		
		DATA.TABLE

			R에서 가장 많이 사용하는 데이터 핸들링 패키지 중 하나로 대용량 데이터의 탐색, 연산, 병합에 유용
			기존 DATA.FRAME보다 월등히 빠른 속도
			특정 COLUMN을 KEY값으로 색인을 지정한 수 데이터를 처리
			빠른 GROUPING과 ORDERING, 짧은 문장 지원 측면에서 데이터 프레임보다 유용함		

		변수의 구간화

			신용평가모형, 고객세분화 등의 시스템으로 모형을 적용하기 위해서 각 변수들을 구간화하여 점수를 적용하는 방식이 활용
			변수의 구간화를 위한 RULE이 존재한다 - 10진수 단위로 구간화, 보통 5개의 구간, 7개 이상의 구간을 만들지 않는다.

			구간화 방법 
				BINNING - 연속형 변수를 범주형 변수로 변환하기 위해 50개 이하의 구간에 동일한 수의 데이터를 할당하여 의미를 파악하면서 구간을 축소하는 방법
				의사결정나무 - 모형을 통해 연속형 변수를 범주형 변수로 변환하는 방법

		결측치 및 이상치 처리

			결측값 처리 
				변수에 데이터가 비어있는 경우 - 9999999, NA, . , UNKNOWN, NOT ANSWER 등으로 표현
				단순 대치법 - 결측값의 레코드를 삭제, 관측 및 실험으로 얻어진 데이터의 평균으로 대체, HOT-DECK, NEAREST-NEIGHBOR 등으로 표준오파의 과소추정문제를 해결한 단순					확률 대치법 사용.
				다중 대치법 - 단순 대치법을 M번 실시하여, M개의 가상적 자료를 만들어 대치
			
			결측값 처리 관련 함수
				COMPLET.CASES() - 결측값이 있으면 FALSE, 없으면 TRUE 반환
				IS.NA() - NA인지의 여부를 TRUE/FALSE로 반환
				DMWR 패키지의 CENTRALIMPUTATION() - NA 값을 가운데 값으로 대치
				DMWR의 KNNIMPUTATION() - NA값을 K최근이웃 알고리즘을 사용하여 대치
				AMELIA 패키지의 AMELIA() 

			이상값 처리
				이상값 - 의도하지 않은 현상으로 입력된 값 OR 의도된 극단값 -> 활용할 수 있음
					잘못 입력된 값 OR 의도하지 않은 현상으로 입력된 값이지만 분석 목적에 부합하지 않는 값 -> BAD DATA이므로 제거

				이상값의 인식
					ESD(EXTREME STUDENTIZED DEVIATION) - 평균으로부터 3표준편차 떨어진 값
					기하평균 - 2.5표준편차 < DATA < 기하평균 + 2.5표준편차
					Q1-1.5(Q3-Q1) < DATA < Q1+1.5(Q3-Q1) 를 벗어나는 데이터

				이상값의 처리
					절단 - 이상값이 포함된 레코드를 삭제
					조정 - 이상값을 상한 또는 하한 값으로 조정


	통계분석

		통계

			통계 - 특정집단을 대상으로 수행한 조사나 실험을 통해 나온 결과에 대한 요약된 형태의 표현
			자료의 획득 방법 - 총 조사와 표본조사
			표본 추출 방법 - 단순랜덤추출, 계통추출, 집락추출, 층화추출

				단순랜덤 추출법 - 각 샘플에 번호를 부여하여 임의의 n개를 추출하는 방법으로, 각 샘플이 선택될 확률은 동일
				계통추출법 - 단순랜덤 추출법의 변형된 방식으로 번호를 부여한 샘플을 나열하여 K개씩 n개의 구간으로 나누고 첫 구간에서 하나를 임의로 선택, K개씩 띄어서 n개를 추출
				집락추출법 - 군집을 구분하고 군집별로 단순랜덤추출법을 시행한 후, 모든 자료를 활용하거나 샘플링 하는 방법
				층화추출법 - 이질적인 원소들로 구성된 모집단에서 각 계층을 고루 대표할수 있는 표본을 추출. 유사한 원소끼리 몇 개의 층을 나누어 각 층에서 랜덤추출

			자료의 측정 방법 - 명목척도, 순서척도, 구간척도, 비율척도

				질적척도 	명목척도 - 어느 집단에 속하는지 분류(성별, 출생지)
					순서척도	- 서열관계를 측정하는 척도(만족도, 선호도, 학년, 신용등급)
				양적척도	구간척도 - 속성의양을 측정하는 것으로 구간이나 구간 사이의 간격이 의미있는 자료(온도, 지수)
					비율척도 - 간격에 대한 비율이 의미를 가지는 자료로 절대적인 기준인 0 이 존재하고 사칙연산이 가능(무게, 나이, 시간, 거리)

		통계 분석

			기술통계 - 평균, 표준편차, 중위수, 최빈값, 그래프
			통계적 추론 - 모수추정, 가설검증, 예측

		확률 및 분포

			확률분포 - 특정 값이 나타날 가능성이 확률적으로 주어지는 변수
			이산형 확률분포 - 베르누이 분포, 이항분포, 기하분포, 다항분포, 포아송분포
			연속형 확률분포 - 균일분포, 정규분포, 지수분포, t-분포, F-분포, 카이제곱 분포

				t-분포 - 두 집단의 평균이 동일한지 알고자 할 때 사용
				카이제곱 분포 - 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용
				F-분포 - 두 집단간 분산의 동일성 검정에 사용

		추정 및 가설검증

			추정 - 표본으로부터 미지의 모수를 추측하는 것
				점추정 - 모수가 특정한 값일 것이라고 추정하는 것. 조건 : 불편성, 효율성, 일치성, 충족성
				구간추정 - 점추정을 보완하기 위해 모수가 특정 구간에 있을 것이라고 추정하는 것. 
					모분산을 알거나 대표본의 경우에는 표준정규분포를 활용, 모분산을 모르거나 소표본의 경우 t-분포를 활용
			가설검정 - 모집단에 대한 가설을 설정한 뒤, 그 가설의 채택여부를 결정하는 방법
				귀무가설(NULL HYPHOTHESIS H0)과 대립가설(ALTER HYPHOTHESIS H1)
				1종 오류(알파) - 귀무가설이 옳은데도 귀무가설을 기각하고 대립가설을 선택할 확률
				2종 오류 (베타)- 귀무가설이 옳지 않고 대립가설이 옳은데도 귀무가설을 선택할 확률

		비모수 검정

			비모수 검정 - 모집단의 분포에 대해 아무런 제약을 가하지 않고 검정을 실시
			가설 설정 방법 - 분포의 형태가 동일하다/동일하지 않다. 등의 식으로 가설 설정
			검정 방법 - 순위나 두 관측값 차이의 부호를 이용해 검정
						
		기술 통계

			기술통계 - 자료의 특성을 표, 그림, 통계량 등을 사용해 쉽게 파악할 수 있도록 정리/요약하는 것

			통계량에 의한 자료 정리
				중심 위치의 측도 - 평균, 중앙값, 최빈값
				산포의 측도 - 분산, 표준편차, 범위, 사분위수범위, 변동계수
				분포의 형태 - 왜도, 첨도

			그래프를 통한 자료 정리
				범주형 자료 - 막대그래프, 파이차트, 모자이크 플랏
				연속형 자료 - 히스토그램, 줄기-잎 그림, 상자그림

		인과관계의 이해

			용어 - 종속변수(반응변수, Y), 독립변수(설명변수, X), 산점도
				-> 산점도에서 두 변수 사이에 선형관계/함수관계가 성립하는지, 이상값의 존재여부와 몇 개의 집단으로 구분되는지 등 확인 가능

			공분산 - 두 변수간의 상관 정도를 상관계수를 통해 확인할 수 있음 -> COV(X,Y)

		상관분석 

			정의와 특성
				두 변수 간의 관계를 상관계수를 통해 알아보는 분석 방법
				1에 가까울 수록 강한 양의 상관관계, -1에 가까울수록 강한 음의 상관관계를 가짐
				상관계수가 0인 경우 데이터 간의 상관이 없음

			유형
				피어슨 상관관계 - 등간척도 이상으로 측정된 두 변수의 상관관계(연속형 변수, 정규성 가정)
				스피어만 상관관계 - 순서, 서열 척도인 두 변수들 간의 상관관계(순서형 변수, 비모수적 방법)
				
				-> COR(X,Y, METHOD = C("")) : PEARSON이 DEFAULT값

		회귀분석

			정의 - 하나 또는 그 이상의 독립변수들이 종속변수에 미치는 영향을 추정할 수 있는 통계 기법
				Yi = B0 + BiXi + Ei의 형태
				독립 변수가 1개인 경우 단순선형회귀분석, 독립변수가 2개 이상인 경우 다중선형회귀분석
				최소제곱법 - 측정값을 기초로 제곱 합을 만들고 그것의 최소인 값을 구하여 처리하는 방법. 잔차 제곱이 가장 작은 선을 선택

			회귀분석의 검정
				회귀식에 대한 검정 -> F-검정
				회귀계수들에 대한 검정 -> t-검정
				모형의 설명력은 결정계수 R^2로 알수 있으며 R^2 = SSR(회귀제곱합)/SST(전체제곱합)
				단순회귀분석의 결정계수는 상관계수값의 제곱과 같음

			선형회귀분석
				가정
					선형성 - 입력변수과 출력변수의 관계가 선형
					독립성 - 잔차와 독립변인은 관련이 없음
					등분산성 - 독립변인의 모든 값에 대한 오차들의 분산이 일정
					비상관성 - 관측치들의 잔차들끼리 상관관계가 없어야 함
					정상성 - 잔차항이 정규분포를 이뤄야 함
				
					-> 다중선형회귀분석의 다중공선성 - 다중회귀분석에서는 설면변수들 사이에 선형관계가 존재하면 회귀계수의 정확한 추정이 곤란
					가중공선성 검사 방법 : 분산팽창요인(VIF)가 10보다 크면 심각, 상태지수가 10이상이면 문제가 있고 30이상이면 심각하다고 판단
			회귀분석의 종류
				단순회귀
				다중회귀
				로지스틱회귀
				다항회귀
				곡선회귀
				비선형회귀

			변수선택법
				모든 가능한 조합 - 모든 가능한 독립변수들의 조합에 대한 회귀모형을 분석해 가장 적합한 모형을 선택
				전진선택법 - 절편만 있는 상수모형으로부터 시작해 중요하다고 생각되는 설명변수부터 차례로 모형에 추가
					-> 이해가 쉽고 많은 변수에 활용이 가능. 변수값의 작은 변동에도 결과가 달라져 안정성이 부족
				후진소거법 - 독립변수 후보 모두를 포함한 모형에서 가장 적은 영향을 주는 변수부터 하나씩 제거
					-> 전체 변수들의 정보를 이용 가능, 변수가 많은 경우 활용이 어렵고 안정성이 부족
				단게별방법 - 전진선택법에 의해 변수를 추가하면서 새롭게 추가된 변수에 기인해 기존 변수가 그 중요도가 약화되면 해단 변수를 제거하는 등 단계별로 추가 또는 삭제되					는 변수를 검토해 더 이상 없을 때 중단

		시계열 자료

			개요
				시계열 자료 - 시간의 흐름에 따라 관찰된 값들
				시계열 데이터의 분석 목적 - 미래의 값을 예측, 특성 파악(경향, 주기, 계절성, 불규칙성)

			정상성
				평균이 일정(모든 시점에서 일정한 평균을 가짐)
				분산도 일정
				공분산도 특정시점 T,S에 의존하지 않고 일정

				평균이 일정하지 않은 경우 차분을 통해 정상화
				분산이 일정하지 않은 경우 변환을 통해 정상화				

			시계열 모형
				자기회귀모형(AR) - P시점 전의 자료가 현재 자료에 영향을 주는 모형
					-> ACF는 빠르게 감소, PACF는 절단점이 존재. AR(절단점-1)로 계산
				이동평균모형(MA) - 같은 시점의 백색잡음과 바로 전 시점의 백색잡음의 결합으로 이루어진 모형
					-> ACF는 절단점이 존재, PACF는 빠르게 감소. 
				자기회귀누적이동평균모형(ARIMA(P,D,Q))
					-> D(차분)=0이면 정상성 만족, P=0이면 D번차분한 MA(Q)모델, Q=0이면 D번 차분한 AR(Q)모델

			분해시계열 
				시계열에 영향을 주는 일반적인 요인을 시계열에서 분리해서 분석하는 방법
				추세요인 - 형태가 오르거나 또는 내리는 추세, 선형, 이차식, 지수형태
				계절요인 - 요일, 월, 사분기 별로 변화하여 고정된 주기에 따라 자료 변화
				순환요인 - 명백한 경제적, 자연적인 이유없이 알려지지 않은 주기로 자료가 변화
				불규칙요인 - 위 세 가지 요인으로 설명할 수 없는 회귀분석에서 오차에 해당하는 요인

		다차원 척도법과 주성분분석

			다차원 척도법

				정의 및 목적
					군집분석과 같이 개체들을 대상으로 변수들을 측정한 후, 개체들 사이의 유사성/비유사성을 측정하여 개체들을 2차원 또는 3차원 공간상에서 점으로 표현하는 분							석방법
					목적 - 개체들의 비유사성을 이용하여 2차원 공간상에 점으로 표시하고 개체들 사이의 집단화를 시각적으로 표현
				방법
					개체들의 거리 계산은 유클리드 거리행렬을 활용
					STRESS - 개체들을 공간상에 표현하기 위한 방법으로 STRESS나 S-STRESS를 부적합도 기준으로 사용
						-> 최적모형의 적합은 부적합도를 최소로 하는 방법으로 일정 수준 이하로 될 때까지 반복해서 수행
				종류
					계량적 MDS - 데이터가 구간척도나 비율척도인 경우 활용. N개의 케이스에 대해 P개의 특성변수가 있는 경우, 각 개체들 간의 유클리드 거리행렬을 계산하고 개						체들 간의 비유사성 S(거리제곱 행렬의 선형함수)를 공간상에 표현
					비계량적 MDS - 데이터가 순서척도인 경우 활용, 개체들 간의 거리가 순서로 주어진 경우에는 순서척도를 거리의 속성과 같도록 변환하여 거리를 생성한 후 적용

			주성분 분석

				정의 및 목적
					상관관계가 있는 변수들을 결합해 상관관계가 없는 변수로 분산을 극대화하는 변수로, 선형결합으로 변수를 축약, 축소하는 기법
					목적 - 여러 변수들을 소수의 주성분으로 축소하여 데이터를 쉽게 이해하고 관리
						주성분분석을 통해 차원을 축소하여 군집분석에서 군집화 결과와 연산속도 개선, 회귀분석에서 다중공선성 최소화

				주성분분석과 요인분석
					요인분석 - 생성된 변수의 수를 지정할 수 없고 이름을 붙일 수 있다. 생성된 변수들이 기본적으로 대등한 관계이고 목표변수를 고려하지 않고 주어진 변수들 간 						비슷한 성격들을 묶음
					주성분분석 - 제1주성분, 제2주성분 을 생성. 순서대로 중요함. 목표변수를 고려하여 주성분 변수 생성

				주성분의 선택법
					누적기여율이 85%이상이면  주성분의 수로 결정할 수 있음.
						-> 제1주성분의 기여율이 0.5311이고 제2주성분이 0.3211이면 제2주성분까지의 누적기여율이 0.8734로 제2주성분까지 선택
						SCREEN PLOT에서 고유값이 수평을 유지하기 전 단계로 주성분의 수를 선택

	정형 데이터 마이닝

		데이터 마이닝

			개요
				정의 - 대용량 데이터에서 의미 있는 패턴을 파악하거나 예측하여 의사결정에 활용하는 방법
				통계분석과의 차이점 - 가설이나 가정에 따른 분석, 검증을 하는 통계분석과는 달리 데이터 마이닝은 다양한 수리 알고리즘을 이용해 데이터베이스의 데이터로부터 의미있						는 정보를 추출
				활용 분야 - 분류, 예측, 군집화, 시각화
				방법론 - 의사결정나무, 로지스틱 회귀분석, 최근접 이웃법, 군집분석, 연관규칙분석

			분석 방법
				교사지도 - 의사결정나무, 인공신경망, 로지스틱 회귀분석, 최근접이웃법, 사례기본 추론
				비교사 지도 - OLAP, 연관규칙분석, 군집분석, SOM

			데이터 마이닝 추진단계
				목적설정 - 데이터마이닝을 위한 명확한 목적 설정
				데이터 준비 - 모델링을 위한 다양한 데이터를 준비, 데이터 정제를 통해 품질을 보장
				데이터 가공 - 목적변수 정의, 모델링을 위한 데이터 형식으로 가공
				기법 적용 - 데이터 마이닝 기법을 적용하여 정보를 추출
				검증 - 마이닝으로 추출한 결과를 검정하고 업무에 적용해 기대효과를 전파

			데이터 분할
				구축용(TRAIN DATA) - 50%의 데이터를 모델링을 위한 훈련용으로 활용
				검정용(VALIDATION DATA) - 30%의 데이터를 구축된 모형의 과대/과소 추정의 판정을 목적으로 활용
				시험용(TEST DATA) - 20%의 데이터를 테스트데이터나 과거 데이터를 활용하여 모델의 성능 평가에 활용

			모델의 성능 평가
				오분류에 대한 추정치
				ROCR 패키지로 성과분석
				이익도표

		분류분석과 예측분석

			분류분석과 예측분석

				공통점 - 레코드의 특정 속성 값을 미리 알아 맞히는 것
				차이점 - 분류는 레코드(튜플)의 범주형 속성 값을 알아 맞히는 것
					예측은 레코드(튜플)의 연속형 속성 값을 알아 맞히는 것
				분류의 예 - 내신등급을 예측, 신용등급을 예측
				예측의 예 - 수능 점수를 예측, 연 매출액을 예측
				분류 모델링 - 신용평가 모형, 사기방지 모형, 이탈모형, 고객세분화
				분류기법 - 로지스틱 회귀분석, 의사결정나무, CART, C5.0, 베이지안 분류, 인공신경망, 지지도벡터기계, K최근접이웃, 규칙기반의 분류와 사례기반추론

			의사결정나무
				정의 - 분류함수를 의사결정 규칙으로 이뤄진 나무 모양으로 그리는 방법으로, 의사결정 문제를 시각화해 의사결정이 이뤄지는 시점과 성과를 한눈에 볼 수 있게 함

				특징
					계산 결과가 의사결정나무에 직접 나타나게 되어 분석이 간편
					분류 정확도가 좋음
					계산이 복잡하지 않아 대용량데이터에서도 빠르게 만들 수 있음
					비정상 잡음 데이터에 대해서도 민감함 없이 분류
					한 변수와 상관성이 높은 다른 불필요한 변수가 있어도 크게 영향을 받지 않음

				활용 
					세분화 - 데이터를 비슷한 특성을 갖는 몇 개의 그룹으로 분할해 그룹별 특성 발견
					분류 - 관측개체를 여러 예측변수들에 근거해 목표변수의 범주를 몇 개의 등급으로 분류하고자 하는 경우
					예측 - 자료에서 규칙을 찾아내고 이를 이용해 미래의 사건을 예측하고자 하는 경우
					차원축소 및 변수 선택 - 매우 많은 수의 예측변수 중 목표변수에 영향을 미치는 변수들을 골래내고자 하는 경우
					교호작용효과의 파악 - 여러개의 예측변수들을 결합해 목표변수에 작용하여 파악하고자 하는 경우
					범주의 병합 또는 연속형 변수의 이산화 - 범주형 목표변수의 범주를 소수의 몇 개로 병합하거나 연속형 목표변수를 몇개의 등급으로 이산화 하고자 하는 경우

				의사결정나무의 분석 과정
					분석단계 : 1. 성장 -> 2. 가지치기 -> 3. 타당성 평가 -> 4. 해석 및 예측

					가지치기 - 너무 큰 나무 모형은 자료를 과대적합하고 너무 작은 나무 모형은 과소적합할 위험이 있어 마디에 속한 자료가 일정 수 이하일 경우, 분할을 							정지하고 가지치기 실시	
					불순도에 따른 분할 측도 - 카이제곱 통계량, 지니계수, 엔트로피 지수

				의사결정 나무의 종류
					CART - 목적변수가 범주형인 경우 지니계수, 연속형인 경우 분산을 이용해 이진분리를 사용.
						개별 입력 뿐만 아니라 입력변수들의 선형결합들 중 최적의 분리를 찾을 수 있음
					C4.5와 C5.0 - 다지분리가 가능하고 범주형 입력변수의 범주 수 만큼 분리 가능. 불순도의 측도로 엔트로피 지수 사용
					CHAID - 가지치기를 하지 않고 적당한 크기에서 나무모형의 성장을 중지하며 입력변수가 반드시 범주형 변수여야함.
						불순도의 측도로 카이제곱 통계량을 사용

			앙상블 기법
				
				개요 
					주어진 자료로부터 여러개의 예측모형들을 만든 후 조합하여 하나의 최종 예측모델을 만드는 방법
					다중 모델 조합, CLASSIFIER COMBINATION 방법이 있음
					학습방법의 불안정성을 해결하기 위해 고안된 기법
					가장 불안정성을 가지는 기법은 의사결정나무, 가장 안정성을 가지는 기법은 1- NEAREST NEIGHBOR

				기법의 종류
					배깅 - 여러개의 붓스트랩 자료를 생성하고 각 붓스트랩 자료의 예측모형 결과를 결합하여 결과를 선정
						훈련자료를 모집단으로 생각하고 평균 예측모형을 구한 것과 같아 분산을 줄이고 예측력을 향상시킬 수 있음
					부스팅 - 예측력이 약한 모형들을 결합하여 강한 예측모형을 만드는 방법
						훈련오차를 빨리 그리고 쉽게 줄일 수 있고, 예측오차의 향상으로 배깅에 비해 뛰어난 예측력을 보임
					랜덤 포레스트 - 의사결정나무의 특징인 분산이 크다는 점을 고려하여 배깅과 부스팅보다 더 많은 무작위성을 주어 약한 학습기들을 생성한 후 이를 선형 결합하						여 최종 학습기를 만드는 방법
						이론적으로 설명이나 해석이 어렵다는 단점이 있지만 예측력이 매우 높은 장점이 있음. 입력변수가 많은 경우 더 높은 예측력을 보임

			성과분석

				오분류표를 통한 모델평가
				
				ROC
					민감도와 1-특이도를 활용하여 모형을 평가
					AUROC(ROC 커브 밑부분의 넓이) = (AR+1)/2 -> 0.9.이상인 경우 EXCELLENT

`			인공신경망

				신경망의 연구 - 인공신경망은 뇌를 기반으로 한 추론모델
				
				뉴런 
					인공신경망은 뉴런이라는 아주 단순하지만 복잡하게 연결된 프로세스로 이루어져 있음
					뉴런은 가중치가 있는 링크들로 연결되어있지만, 뉴런은 여러 개의 입력신호를 받아 하나의 출력신호를 생성
					뉴런은 전이함수, 즉 활성화함수를 사용
						-> 뉴런은 입력 신호의 가중치 합을 계산하여 임계값과 비교
						가중치 합이 임계값보다 작으면 뉴런의 출력은 -1, 같거나 크면 1을 출력함

					활성화 함수의 종류 - 계단함수, 부호함수, 시그모이드함수, RELU함수

				신경망모형 구축 시 고려사항

					입력변수 - 신경망 모형은 복잡성으로 인해 입력자료의 선택에 매우 민감함
						-> 범주형변수 - 각 범주의 빈도가 일정수준 이상이고 각 범주의 빈도가 일정할 때 활용
						연속형 변수 - 입력 값의 범위가 변수들간에 큰 차이가 없을 때 활용, 분포가 대칭이 아니면 좋지 않은 결과 도출 -> 변환 또는 범주화 활용

					가중치 초기값 -  역전파 알고리즘의 경우, 초기값에 따라 결과가 많이 달라져 초기값 선택이 매우 중요
						가중치가 0이면 시그모이드 함수는 선형이 되고 신경망 모형도 선형모형이 됨
						초기값은 0 근처의 랜덤값으로 선정하고 초기에는 선형모형에서 가중치가 증가하면서 비선형으로 변경됨
				
					예측값 선정 - 비용함수 R는 비볼록함수이고 여러개의 국소 최소값들을 가짐
						랜덤하게 선택된 여러 개의 초기값에 대한 신경망을 적합한 후 얻은 해들을 비교하여 가장 오차가 작은 것을 선택하여 최종 예측값을 얻거나 평균을 구						하여 최종 예측값으로 선정
						훈련자료에 대하여 배깅을 적용하여 최종 예측치를 선정
					
					학습률 - 상수값을 사용하며, 처음에는 큰 값으로 정하고 반복이 진행되어 해가 가까울수록 0에 수렴
				
					은닉측, 은닉 노드의 수
						은닉측과 은닉노드가 많으면 - 가중치가 많아져서 과대적합문제가 발생
						은닉측과 은닉노드가 적으면 - 과소적합 문제 발생
						은닉층 수 결정 - 은닉층이 하나인 신경망은 범용근사자이므로 가급적이면 하나로 선정
						은닉노드 수 결정 - 적절히 큰 값으로 결정하고 가중치를 감소하면서 모수에 대한 벌점화 적용
					
					과대 적합 문제 - 신경망은 많은 가중치를 추정해야 하므로 과대적합 문제가 빈번
						해결방안 -> 조기종료 또는 선형모형의 능형회귀와 유사한 가중치 감소라는 벌점화 기법 활용

			로지스틱 회귀분석
				반응 변수가 범주형인 경우에 적용되는 회귀분석 모형
				새로운 설명변수가 주어질 때 반응변수의 각 범주에 속할 확률이 얼마인지를 추정하여 추정 확률을 기준치에 따라 분류하는 목적으로 활용
				이때 모형의 적합을 통해 추정된 확률을 사후확률이라고 함
				X1이 한 단위 증가할때마다 성공의 오즈가 몇배 증가하는지를 확인
				GLM() 함수를 활용하여 로지스틱 회귀분석을 실행
					-> GLM(종속변수 ~ 독립변수1 + .... + 독립변수K, FAMILY=BINOMIAL, DATA = 데이터셋명)
					분석결과 베타의 추정값이 5.14이면, 독립변수의 단위가 증가함에 따라 종속변수가 0에서1로 바뀔때 오즈가 EXP(5.17)=약170배 증가한다는 의미

			
		군집분석

			개요
				각 개체간의 유사성을 측정하여 유사성이 높은 대상집단을 분류하고, 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 객체간의 상이성을 규명하는 분석방법
				특성에 따라 고객을 여러 개의 배타적인 집단으로 나누는 것으로 군집의 개수, 구조에 대한 가정 없이 데이터로부터 거리 기준으로 군집화 유도
	
			특징
				비교사학습법에 해당하여 타겟변수(종속변수)의 정의가 없이 학습이 가능
				데이터를 분석의 목적에 따라 적절한 군집으로 분석자가 정의 가능
				요인분석과의 차이 - 유사한 변수를 함께 묶어주는 목적이 아니라 각 데이터를 묶어줌
				판별분석과의 차이 - 판별분석은 사전에 집단이 나누어져있어야 하지만 군집분석은 집단이 없는 상태에서 집단을 구분

			거리 측정 방법
				연속형 변수 - 유클리드 거리, 표준화 거리, 맨하탄 거리, 민코우스키 거리, 마할라노비스, 체비셔프, 캔버라
				범주형 변수 - 자카드 거리, 코사인 거리

			계층적 군집분석
				최단연결법 - NN 거리 행렬에서 거리가 가장 가까운 데이터를 묶어서 군집 형성
					군집과 군집 또는 데이터와의 거리를 계산시 최단거리를 거리로 계산하여 거리행렬 수정
					수정된 거리행렬에서 거리가 가까운 데이터 또는 군집을 새로운 군집으로 형성
				최장연결법 - 거리를 계산기 최장거리를 거리로 계산하여 거리행렬을 수정
				평균연결법 - 평균거리를 계산하여 거리행렬 수정
				와드연결법 - 군집내 편차들의 제곱합을 고려한 방법으로 군집간 정보의 손실을 최소화하기 위해 군집화를 진행

			비계층적 군집분석
				N개의 개체를 G개의 군집으로 나눌 수 있는 모든 가능한 방법을 점검해 최적화한 군집을 형성하는 것
				K-평균 군집분석
					1. 원하는 군집의 개수와 초기값들을 정해 초기값을 중심으로 군집을 형성
					2. 각 데이터를 거리가 가장 가까운SEED가 있는 군집으로 분류
					3. 각 군집의 SEED값을 다시 계산
					4. 모든 개체가 군집으로 할당될 때 까지 위 과정을 반복
				장점 - 주어진 데이터 내부구조에 대한 사전정보 없이 의미있는 자료 도출, 다양한 형태의 데이터에 적용 가능, 분석방법 적용이 용이함
				단점 - 가중치와 거리정의가 어려움, 초기 군집수를 정하기 어려움, 사전에 주어진 목적이 없으므로 결과 해석이 어려움

			혼합 분포 군집
				모형기반의 군집 방법이며, 데이터가 K개의 모수적 모형의 가중함으로 표현되는 모집단의 모형으로부터 나왔다는 가정하에서 모수롸 함께 가중치를 자료로부터 추정하는 				방법을 사용
				K개의 각 모형은 군집을 의미하며, 각 데이터는 추정된 K개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집의 분류가 이루어짐
				흔히 혼합모형에서는 모수와 가중치 추정 -> EM알고리즘 사용

				특징
					K평균군집의 절차와 유사하지만 확률분포를 도입하여 군집을 진행
					군집을 몇 개의 모수로 표현할 수 있으며, 서로 다른 크기나 모양의 군집을 찾을 수 있음
					EM알고리즘을 이용한 모수 추정에서 데이터가 커지면 수렴에 시간이 걸림
					K평균군집과 같이 이상치 자료에 민감하므로 사전 조치 필요
					군집의 크기가 너무 작으면 추정의 정도가 떨어지거나 어려움

			SOM(SELF ORGANIZING MAP)
				비지도 신경망으로 고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬하여 지도 형태로 형상화. 
				이러한 형상화는 입력변수의 위치관계를 그래도 보존. 실제 공간의 입력변수가 가까이 있으면 지도상에 가까운 위치에 있다는 것을 의미
				
				특징
					고차원의 데이터를 저차원의 지도 형태로 형상화 하기 때문에 시각적으로 이해가 쉬움
					입력변수의 위치관게를 그대로 보존하기 때문에 실제 데이터가 유사하면 지도상에서 가갑게 표현되며, 이런 특징 때문에 패턴 발견, 이미지 분석 등에서 뛰어난 성					능을 보임
					역전파 알고리즘등을 이용하는 인공신경망과 달리 단 하나늬 전방패스를 사용함으로써 속도가 매우 빠르고 실시간 학습처리를 할 수 있는 모형


		연관분석

			개요
				기업의 데이터베이스에서 상품의 구매, 서비스 등 일련의 거래 또는 사건들 간의 규칙을 발견하기 위한 분석
				흔히 장바구니 분석, 순차분석 등이 있음
				장바구니 분석 - 장바구니에 무엇이 같이 들어있는지에 대한 분석
				순차 분석 - 구매 이력을 분석해서 A품목을 산 후 추가 B품목을 사는지를 분석

			형태 
				조건과 반응의 형태(IF-THEN)

			측도
				지지도 - 전체 거래 항목 A와 항목 B를 동시에 포함하는 거래의 비율
				신뢰도 - 항목 A를 포함한 거래중에서 A와 항목B가 같이 포함될 확률. 연관성의 정도를 파악할 수 있음
				향상도 - A가 주어지지 않았을 때의 품목 B의 비율에 비해 A가 주어졌을때 품목 B의 확률의 증가 비율
					-> 연관규칙 A->B는 품목 A와 품목B의 구매가 서로 관련이 없는 경우에 향상도가 1이 됨

			특징
				절차
					최소지지도 선정(5프로이상) -> 최소지지도를 넘는 품목 분류 -> 두가지 품목 집합 생성 -> 반복 수행으로 빈발 품목 집합 선정
				장점
					탐색적인 기법 - 조건반응으로 표현되는 연관성분석 결과를 쉽게 이해할 수 있음
					강력한 비목적성 분석기법 - 분석방향이나 목적이 특별이 없는 경우 목적변수가 없으므로 유용하게 활용됨
					사용이 편리한 분석데이터의 형태 - 거래 내용에 대한 데이터를 변환없이 그 자체로 이용
					계산의 용이성 - 분석을 위한 계산이 상당히 간단
				단점
					상당한 수의 계산과정 - 품목 수가 증가하면 분석에 필요한 계산이 기하급수적으로 증가
					적절한 품목의 결정 - 너무 세분화된 품목을 갖고 연관성 규칠을 찾으면 수많은 연관성 규칙들이 발견되지만, 실제로 발생 비율 면에서 의미없는 분석이 될수 있음
					품목과 비율차이 - 사용될 모든 품목들 자체가 전체자료에서 동일한 빈도를 갖는 경우, 연관성 분석은 가장 좋은 결과를 얻음. 그러나 거래량이 적은 품목은 당연히 						포함된 거래수가 적고 규칙 발견 과정 중에서 제외되기 쉬움

			평가기준 적용시 주의점
				두 항목의 신뢰도가 높다고 해서 꼭 두 항목이 높은 연관관계가 있는 것은 아님(지지도를 고려) -> 지지도가 낮다면 구매율 자체가 낮으므로 상관관계로 보기 어려움
				지지도와 신뢰도가 높게 나왔더라도 꼭 높은 연관관계가 있는 것은 아님(향상도를 고려) -> 단순히 빈번하게 구매되는 항목일 수 있음
				두 항목의 신뢰도가 높게 나왔을때, 전체거래에서 B자체의 구매율보다 A자체의 구매율이 더 높아야 의미있는 정보

			APRIORI 알고리즘
				어떤 항목이 빈발한다면, 그 항목집합의 모든 부분집합도 빈발
















		