
Data type constraints

    - df['column'].str.strip('') : 해당 문자를 제거한다.
    - df['column'].astype('int') : 데이터 타입을 변경한다.

        -> category형인 경우, 'category'를 사용한다.
        .describe()를 사용하면, 다른형태의 요약통계값들을 얻을 수 있다.

    - datetype을 확인할 때, type()을 사용해도 되고, .dtype을 사용해도 된다.

    assert 조건 == 값 ?

        -> 참인경우 아무값도 반환하지 않고, 거짓인 경우 assertionerror가 발생한다.

----------------------------------------------------------------------

In this exercise, and throughout this chapter, you'll be working with bicycle ride
sharing data in San Francisco called ride_sharing. It contains information on the start
and end stations, the trip duration, and some user information for a bike sharing service.

The user_type column contains information on whether a user is taking a free ride and
takes on the following values:

1 for free riders.
2 for pay per ride.
3 for monthly subscribers.
In this instance, you will print the information of ride_sharing using .info() and see a
firsthand example of how an incorrect data type can flaw your analysis of the dataset.
The pandas package is imported as pd.

1 - Convert user_type into categorical by assigning it the 'category' data type and store it in the user_type_cat column.
2 - Make sure you converted user_type_cat correctly by using an assert statement.

    # Print the information of ride_sharing
    print(ride_sharing.info())

    # Print summary statistics of user_type column
    print(ride_sharing['user_type'].describe())

    # Convert user_type from integer to category
    ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')

    # Write an assert statement confirming the change
    assert ride_sharing['user_type_cat'].dtype == 'category'

    # Print new summary statistics
    print(ride_sharing['user_type_cat'].describe())

----------------------------------------------------------------------

In the previous exercise, you were able to identify that category is the correct data
type for user_type and convert it in order to extract relevant statistical summaries
that shed light on the distribution of user_type.

Another common data type problem is importing what should be numerical values as strings,
as mathematical operations such as summing and multiplication lead to string concatenation,
not numerical outputs.

In this exercise, you'll be converting the string column duration to the type int. Before
that however, you will need to make sure to strip "minutes" from the column in order to
make sure pandas reads it as numerical. The pandas package has been imported as pd.

1 - Use the .strip() method to strip duration of "minutes" and store it in the duration_trim column.
2 - Convert duration_trim to int and store it in the duration_time column.
3 - Write an assert statement that checks if duration_time's data type is now an int.
4 - Print the average ride duration.

    # Strip duration of minutes
    ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip("minutes")

    # Convert duration to integer
    ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')

    # Write an assert statement making sure of conversion
    assert ride_sharing['duration_time'].dtype == 'int'

    # Print formed columns and calculate average ride duration
    print(ride_sharing[['duration','duration_trim','duration_time']])
    print(ride_sharing['duration_time'].mean())

----------------------------------------------------------------------

How to deal with out of range data

    - Dropping data

        해당 범위내의 값들로만 새로운 데이터셋을 생성한다.

            ex) movies = movies[movies['avg_rating'] <= 5]

        혹은 해당 값들을 제거한다.

            ex) movies.drop(movies[movies['avg_rating'] > 5].index, inplace = True)

                -> 평점이 5보다 큰 영화의 행 인덱스를 인수로 받아서 해당 인덱스를 제거한다.

        필수 정보를 잃게 될 수도 있다.

    - Setting custom minimums and maximums

        초과치를 최대치로 변환

            ex) movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5

    - Treat as missing and impute

    - Setting custom value depending on business assumptions

        * convert to datetime

        pd.to_datetime을 사용용
        dt.date.today()를 통해 오늘의 날짜값을 받아온다.

****** 데이터 프레임에서 값을 변경할 때, .loc를 사용하는데
    첫번째 인수는 행, 두번째 인수 열을 받는데, movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5
    와 같은 형태로 변경할 수 있다.
    매우 유용한 테크닉인거 같으니 꼭 명심하자.

****** 중복값의 처리

    .duplicated() : 중복값의 존재 여부를 확인한다.
        값들을 참과 거짓의 Series로 반환한다.

    .drop_duplicates() : 중복값을 제거한다.

    -> 두 항목 모두에 대해서, keep = ''을 통해 어떤 값들을 중복값으로 볼 것인지 선택할 수 있다.

        keep = 'first' 라면 첫번째 값을 유지하고, 다음값부터 중복값으로 처리.
        keep = 'last' 라면 마지막 값을 유지하고, 다음값부터 중복으로 처리.
        keep = False 라면 겹치는 값들을 모두 중복으로 처리. 값들을 남기지 않는다.

    -> subset을 통해 어떤 컬럼에 대해서 중복값을 찾고 제거할 것인지 정한다.

    sort_values(by = '')를 통해 같은 값들끼리 배열 할 수 있다. -> 중복값의 존재 여부 확인인

    -> 모든값이 일치하지는 않는 경우에 대한 처리방법

        .groupby().agg() 를 통해 평균을 사용하던, 최대, 최소등을 선택적으로 사용한다.

        ex) column_names = ['first_name', 'last_name', 'address']
            summaries = {'height':'max', 'weight':'mean'}
            height_weight = height_weight.groupby(by=column_names).agg(summaries).reset_inex

            -> agg를 적용할 때, 컬럼별로 다른 통계를 사용하기 위해 딕셔너리를 생성한다.

--------------------------------------------------------------------

In this lesson, you're going to build on top of the work you've been doing with the ride_
sharing DataFrame. You'll be working with the tire_sizes column which contains data on
each bike's tire size.

Bicycle tire sizes could be either 26″, 27″ or 29″ and are here correctly stored as a
categorical value. In an effort to cut maintenance costs, the ride sharing provider
decided to set the maximum tire size to be 27″.

In this exercise, you will make sure the tire_sizes column has the correct range by first
converting it to an integer, then setting and testing the new upper limit of 27″ for
tire sizes.

1 - Convert the tire_sizes column from category to 'int'.
2 - Use .loc[] to set all values of tire_sizes above 27 to 27.
3 - Reconvert back tire_sizes to 'category' from int.
4 - Print the description of the tire_sizes.

    # Convert tire_sizes to integer
    ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')

    # Set all values above 27 to 27
    ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27

    # Reconvert tire_sizes back to categorical
    ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')

    # Print tire size description
    print(ride_sharing['tire_sizes'].describe())

    -> astype, strip등을 사용할 때, 문자열인 경우는 .str을 사용하지만
    숫자형 카테고리형과 같은 경우에는 .str을 사용하지 않는다.

----------------------------------------------------------------------

A new update to the data pipeline feeding into the ride_sharing DataFrame has been updated
to register each ride's date. This information is stored in the ride_date column of the
type object, which represents strings in pandas.

A bug was discovered which was relaying rides taken today as taken next year. To fix this,
you will find all instances of the ride_date column that occur anytime in the future,
and set the maximum possible value of this column to today's date. Before doing so, you
would need to convert ride_date to a datetime object.

The datetime package has been imported as dt, alongside all the packages you've been
using till now.

1 - Convert ride_date to a datetime object and store it in ride_dt column using to_datetime().
2 - Create the variable today, which stores today's date by using the dt.date.today() function.
3 - For all instances of ride_dt in the future, set them to today's date.
4 - Print the maximum date in the ride_dt column.

    # Convert ride_date to datetime
    ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])

    # Save today's date
    today = dt.date.today()

    # Set all in the future to today's date
    ride_sharing.loc[ride_sharing['ride_dt']>today,'ride_dt'] = today

    # Print maximum of ride_dt column
    print(ride_sharing['ride_dt'].max())

----------------------------------------------------------------------

A new update to the data pipeline feeding into ride_sharing has added the ride_id column,
which represents a unique identifier for each ride.

The update however coincided with radically shorter average ride duration times and
irregular user birth dates set in the future. Most importantly, the number of rides taken
has increased by 20% overnight, leading you to think there might be both complete and inc
omplete duplicates in the ride_sharing DataFrame.

In this exercise, you will confirm this suspicion by finding those duplicates. A sample
of ride_sharing is in your environment, as well as all the packages you've been working
with thus far.

1 - Find duplicated rows of ride_id in the ride_sharing DataFrame while setting keep to False.
2 - Subset ride_sharing on duplicates and sort by ride_id and assign the results to duplicated_rides.
3 - Print the ride_id, duration and user_birth_year columns of duplicated_rides in that order.

    # Find duplicates
    duplicates = ride_sharing.duplicated(subset='ride_id', keep=False)

    # Sort your duplicated rides
    duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')

    # Print relevant columns of duplicated_rides
    print(duplicated_rides[['ride_id','duration','user_birth_year']])

----------------------------------------------------------------------

In the last exercise, you were able to verify that the new update feeding into
ride_sharing contains a bug generating both complete and incomplete duplicated rows for
some values of the ride_id column, with occasional discrepant values for the
user_birth_year and duration columns.

In this exercise, you will be treating those duplicated rows by first dropping complete
duplicates, and then merging the incomplete duplicate rows into one while keeping the
average duration, and the minimum user_birth_year for each set of incomplete duplicate
rows.

1 - Drop complete duplicates in ride_sharing and store the results in ride_dup.
2 - Create the statistics dictionary which holds minimum aggregation for user_birth_year and mean aggregation for duration.
3 - Drop incomplete duplicates by grouping by ride_id and applying the aggregation in statistics.
4 - Find duplicates again and run the assert statement to verify de-duplication.

    # Drop complete duplicates from ride_sharing
    ride_dup = ride_sharing.drop_duplicates()

    # Create statistics dictionary for aggregation function
    statistics = {'user_birth_year': 'min', 'duration': 'mean'}

    # Group by ride_id and compute new statistics
    ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()

    # Find duplicated values again
    duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)
    duplicated_rides = ride_unique[duplicates == True]

    # Assert duplicates are processed
    assert duplicated_rides.shape[0] == 0

    -> 컬럼마다 다른 요약통계를 사용하는 경우에는 딕셔너리를 사용하는 것을 꼭 기억하자.

----------------------------------------------------------------------

Anti joins - 두개의 데이터프레임 A와 B를 가져와, 다른 데이터프레임에 포함되지 않은 값들을 반환

    -> 데이터프레임 A에서만 발견되는 값들을 찾아내는 데에 유용하다.

Inner joins - 두 데이터프레임에서 모두 발견되는 값들을 반환한다.

    - 다음과 같이 사용한다.

        inconsistent_categories = set(study_data['blood_type'].difference(categories['blood_type'])

        -> set을 통해 중복을 제거한 컬럼의 값들을 리스트
        그런 뒤, difference를 통해 다른 Series와의 Anti join을 하여 저장한다.(차집합)

    - isin을 사용하여 해당 컬럼을 가져올 수도 있다.

        inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)
        study_data[inconsistent_rows]

        -> isin()은 해당 값들이 리스트 내에 있는지를 불리언의 형태로 반환한다.

    **** Membership Constraints - 카테고리형 데이터인지를 물어보는 것인듯.
    카테고리형 데이터타입을 위해, 별개의 데이터프레임을 형성하는 것 같다.

----------------------------------------------------------------------

In this exercise and throughout this chapter, you'll be working with the airlines
DataFrame which contains survey responses on the San Francisco Airport from airline
customers.

The DataFrame contains flight metadata such as the airline, the destination, waiting
times as well as answers to key questions regarding cleanliness, safety, and
satisfaction. Another DataFrame named categories was created, containing all correct
possible values for the survey columns.

In this exercise, you will use both of these DataFrames to find survey answers with
inconsistent values, and drop them, effectively performing an outer and inner join on
both these DataFrames as seen in the video exercise. The pandas package has been
imported as pd, and the airlines and categories DataFrames are in your environment.

1 - Print the categories DataFrame and take a close look at all possible correct categories of the survey columns.
2 - Print the unique values of the survey columns in airlines using the .unique() method.

    # Print categories DataFrame
    print(categories)

    # Print unique values of survey columns in airlines
    print('Cleanliness: ', airlines['cleanliness'].unique(), "\n")
    print('Safety: ', airlines['safety'].unique(), "\n")
    print('Satisfaction: ', airlines['satisfaction'].unique(), "\n")

    -> 카테코리형 컬럼에 대해서 unique()를 사용하면, 중복을 제거한 하나의 값들만을 리스트의 형으로
    반환하며, 총 몇개의 값이 있는지도 반환한다.

3 - Create a set out of the cleanliness column in airlines using set() and find the inconsistent category by finding the difference in the cleanliness column of categories.
4 - Find rows of airlines with a cleanliness value not in categories and print the output.

    # Find the cleanliness category in airlines not in categories
    cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])

    # Find rows with that category
    cat_clean_rows = airlines['cleanliness'].isin(cat_clean)

    # Print rows with inconsistent category
    print(airlines[cat_clean_rows])

    -> airlines의 cleanliness 컬럼의 중복을 제거한 리스트로 만들어, 차집합을 통해 겹치지 않는 값을
    cat_clean에 저장한다. 이때 겹치는 값을 제외한 Unacceptable만 저장된다.

    -> 그리고 cleanliness 컬럼의 값이 cat_clean에 포함되어있는지를 불리언의 형태로
    cat_clean_rows에 저장하였다.

    -> 마지막으로 이 값들이 포함되어있는 행들을 출력한다.


5 - Print the rows with the consistent categories of cleanliness only.

    # Find the cleanliness category in airlines not in categories
    cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])

    # Find rows with that category
    cat_clean_rows = airlines['cleanliness'].isin(cat_clean)

    # Print rows with inconsistent category
    print(airlines[cat_clean_rows])

    # Print rows with consistent categories only
    print(airlines[~cat_clean_rows])

    -> airlines 데이터프레임에서, 불리언을 반대로 Unacceptable이 없는 행들을 모두 출력하였다.

----------------------------------------------------------------------

Value consistency

    1) 범주형 데이터들이 대/소문자가 구분이 안되어있는 경우

    ex) 'married', 'Married', 'UNMARRIED', 'unmarried'와 같이 다 다른 값으로 처리를 한다.

    - 특정값의 갯수를 세는 경우,
        Series인 경우에는 .value_counts()
        groupby인 경우에는 .count()

        -> 열의 데이터들을 모두 대문자화 하거나, 소문자화 하는 식으로 처리할 수 있다.

            .str.upper()
            .str.lower()

    2) 선/후행 공백이 있는 경우

    ex) 'married ', 'married', 'unmarried', ' unmarried', 마찬가지로 다른 값으로 처리한다.

    - str.strip()을 통해 선/후행 공백을 제거한다. 디폴트가 공백의 제거.

Collapsing data into categories

    1) 연속형 데이터를 범주형으로 바꾸는 방법.

        ex) group_name = ['0-200K','200K-500K','500K+']
        demographics['income_group'] = pd.qcut(demographics['household_income'], q=3,
        labels = group_names)

        demographics[['income_group', 'household_income']]

        -> 이 경우, 범주를 어떻게 나누는지 설정하지 않았기 때문에 올바른 cut이 되지 않았다.
        그냥 3등분 한다.
        해서 pd.cut()을 사용 사용하는 것이 정확하다.

        ex) ranges = [0, 200000, 500000, np.inf]
        group_name = ['0-200K','200K-500K','500K+']

        demographics['income_group'] = pd.cut(demographics['household_income'],
        bins = ranges, labels = group_names)

        demographics[['income_group', 'household_income']]

        -> bins = ranges를 통해 정확한 cut의 지점을 설정한다.
        np.inf를 통해 무한대 값을 설정하였다.

    2) 큰 카데고리를 작은 카테고리로 변경할 때

        ex) mapping = {} 딕셔너리를 통해 변경할 카테고리를 설정해 주고,
        .replace(mapping)을 통해 변경해 준다.

----------------------------------------------------------------------

In this exercise, you'll be revisiting the airlines DataFrame from the previous lesson.

As a reminder, the DataFrame contains flight metadata such as the airline, the
destination, waiting times as well as answers to key questions regarding cleanliness,
safety, and satisfaction on the San Francisco Airport.

In this exercise, you will examine two categorical columns from this DataFrame,
dest_region and dest_size respectively, assess how to address them and make sure that
they are cleaned and ready for analysis. The pandas package has been imported as pd,
and the airlines DataFrame is in your environment.

1 - Print the unique values in dest_region and dest_size respectively.
2 - Change the capitalization of all values of dest_region to lowercase.
3 - Replace the 'eur' with 'europe' in dest_region using the .replace() method.
4 - Strip white spaces from the dest_size column using the .strip() method.
5 - Verify that the changes have been into effect by printing the unique values of the columns using .unique() .

    # Print unique values of both columns
    print(airlines['dest_region'].unique())
    print(airlines['dest_size'].unique())

    # Lower dest_region column and then replace "eur" with "europe"
    airlines['dest_region'] = airlines['dest_region'].str.lower()
    airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})

    # Remove white spaces from `dest_size`
    airlines['dest_size'] = airlines['dest_size'].str.strip()

    # Verify changes have been effected
    print(airlines['dest_region'].unique())
    print(airlines['dest_size'].unique())

----------------------------------------------------------------------

To better understand survey respondents from airlines, you want to find out if there
is a relationship between certain responses and the day of the week and wait time at
the gate.

The airlines DataFrame contains the day and wait_min columns, which are categorical
and numerical respectively. The day column contains the exact day a flight took place,
and wait_min contains the amount of minutes it took travelers to wait at the gate. To
make your analysis easier, you want to create two new categorical variables:

    wait_type: 'short' for 0-60 min, 'medium' for 60-180 and long for 180+
    day_week: 'weekday' if day is in the weekday, 'weekend' if day is in the weekend.

The pandas and numpy packages have been imported as pd and np. Let's create some new
categorical data!

1 - Create the ranges and labels for the wait_type column mentioned in the description above.
2 - Create the wait_type column by from wait_min by using pd.cut(), while inputting label_ranges and label_names in the correct arguments.
3 - Create the mapping dictionary mapping weekdays to 'weekday' and weekend days to 'weekend'.
4 - Create the day_week column by using .replace().

    # Create ranges for categories
    label_ranges = [0, 60, 180, np.inf]
    label_names = ['short', 'medium', 'long']

    # Create wait_type column
    airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, labels = label_names)

    # Create mappings and replace
    mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', 'Thursday': 'weekday', 'Friday': 'weekday', 'Saturday': 'weekend', 'Sunday': 'weekend'}

    airlines['day_week'] = airlines['day'].replace(mappings)

----------------------------------------------------------------------

Text data

    Inconsistent data format, length violation 등의 문제가 발생한다.

    ex) digits = phones['Phone number'].str.len()
        phones.loc[digits < 10, 'Phone number'] = np.ana

        -> 숫자열의 길이가 10보다 작은 경우, 해당 컬럼의 값을 NaN으로 변경

        sanity_check = phone['Phone number'].str.len()
        assert sanity_check.min() >= 10

        -> 해당 전화번호 열에서 길이의 최소값이 10보다 큰거나 같은지 체크

        assert phone['Phone number'].str.contains("+|-").any() == False

        -> 전화번호 열에 +와 -의 문자를 포함하는 행이 존재하는지 체크
        + 혹은 -을 포함한다면 True를 반환.
        any()는 하나라도 True가 존재한다면 True를 반환

    ***** 정규식을 통한 숫자열 추출

        .str.replace(r'\D+', '')

        -> 모든 문자(숫자가 아닌것들을 모두)를 공백으로 변환

--------------------------------------------------------------------

While collecting survey respondent metadata in the airlines DataFrame, the full name
of respondents was saved in the full_name column. However upon closer inspection, you
found that a lot of the different names are prefixed by honorifics such as "Dr.",
"Mr.", "Ms." and "Miss".

Your ultimate objective is to create two new columns named first_name and last_name,
containing the first and last names of respondents respectively. Before doing so
however, you need to remove honorifics.

The airlines DataFrame is in your environment, alongside pandas as pd.

1 - Remove "Dr.", "Mr.", "Miss" and "Ms." from full_name by replacing them with an empty string "" in that order.
2 - Run the assert statement using .str.contains() that tests whether full_name still contains any of the honorifics.

    # Replace "Dr." with empty string ""
    airlines['full_name'] = airlines['full_name'].str.replace("Dr.","")

    # Replace "Mr." with empty string ""
    airlines['full_name'] = airlines['full_name'].str.replace("Mr.","")


    # Replace "Miss" with empty string ""
    airlines['full_name'] = airlines['full_name'].str.replace("Miss","")


    # Replace "Ms." with empty string ""
    airlines['full_name'] = airlines['full_name'].str.replace("Ms.","")


    # Assert that full_name has no honorifics
    assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False

    -> 제거하려는 문자열들을 리스트에 저장하여 한번에 제거하는 방법은 없는가?
    replace(["Dr.","Mr.","Miss","Ms."],"") 와 같은 형태

----------------------------------------------------------------------

To further understand travelers' experiences in the San Francisco Airport, the quality
assurance department sent out a qualitative questionnaire to all travelers who gave the
airport the worst score on all possible categories. The objective behind this
questionnaire is to identify common patterns in what travelers are saying about the
airport.

Their response is stored in the survey_response column. Upon a closer look, you
realized a few of the answers gave the shortest possible character amount without much
substance. In this exercise, you will isolate the responses with a character count
higher than 40 , and make sure your new DataFrame contains responses with 40 characters
or more using an assert statement.

The airlines DataFrame is in your environment, and pandas is imported as pd.

1 - Using the airlines DataFrame, store the length of each instance in the survey_response column in resp_length by using .str.len().
2 - Isolate the rows of airlines with resp_length higher than 40.
3 - Assert that the smallest survey response length in airlines_survey is now bigger than 40.

    # Store length of each row in survey_response column
    resp_length = airlines['survey_response'].str.len()

    # Find rows in airlines where resp_length > 40
    airlines_survey = airlines[resp_length > 40]

    # Assert minimum survey_response length is > 40
    assert airlines_survey['survey_response'].str.len().min() > 40

    # Print new survey_response column
    print(airlines_survey['survey_response'])

----------------------------------------------------------------------

Uniformity

    Temperature, Weight, date, Money 등 단위의 통일성

    ex) 화씨온도를 섭씨온도로 변경하기.

        temp_fah = temperatures.loc[temperatures['Temperatures']>40, 'Temperature']
        temp_cel = (temp_fah - 32) * (5/9)

        temperatures.loc[temperatures['Temperature']>40, 'Temperature'] = temp_cel

        assert temperatures['Temperature'].max() < 40

    ex) 날짜 데이터 형태

        27/27/19, 03-29-19, March 3rd, 2019

        -> 처음의 월이 27인 값을 제외하고는, pd.to_datetime을 사용하면
        모두 같은 형식으로 바꿔준다.

        pd.to_datetime(df['column'], infer_datetime_format=True, errors='coerce')

            infer_datetime_format을 통해 각각의 날짜값들을 추론한다.
            errors = 'coerce'를 통해 날짜에 대한 누락된 값들을 NaT으로 반환한다.
            (datetime 계열의 NaN)

        .dt.strftime("%d-%m-%Y") 를 통해 날짜 형식을 정해줄 수 있다.

----------------------------------------------------------------------

In this exercise and throughout this chapter, you will be working with a retail banking
dataset stored in the banking DataFrame. The dataset contains data on the amount of
money stored in accounts, their currency, amount invested, account opening date and
last transaction date that were consolidated from American and European branches.

You are tasked with understanding the average account size and how investments vary by
the size of account, however in order to produce this analysis accurately, you first
need to unify the currency amount into dollars. The pandas package has been imported
as pd, and the banking DataFrame is in your environment.

1 - Find the rows of acct_cur in banking that are equal to 'euro' and store them in acct_eu.
2 - Find all the rows of acct_amount in banking that fit the acct_eu condition, and convert them to USD by multiplying them with 1.1.
3 - Find all the rows of acct_cur in banking that fit the acct_eu condition, set them to 'dollar'.

    # Find values of acct_cur that are equal to 'euro'
    acct_eu = banking['acct_cur'] == 'euro'

    # Convert acct_amount where it is in euro to dollars
    banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1

    # Unify acct_cur column by changing 'euro' values to 'dollar'
    banking.loc[acct_eu, 'acct_cur'] = 'dollar'

    # Assert that only dollar currency remains
    assert banking['acct_cur'].unique() == 'dollar'

    -> 화폐단위가 유로인 행들을 불리언의 형태로 acct_eu에 저장.
    해당 행의 총액 값을 1.1로 곱해준 뒤, 단위를 달러로 변경경

---------------------------------------------------------------------

After having unified the currencies of your different account amounts, you want to add a temporal
dimension to your analysis and see how customers have been investing their money given
the size of their account over each year. The account_opened column represents when
customers opened their accounts and is a good proxy for segmenting customer activity
and investment over time.

However, since this data was consolidated from multiple sources, you need to make sure
that all dates are of the same format. You will do so by converting this column into a
datetime object, while making sure that the format is inferred and potentially incorrect
formats are set to missing. The banking DataFrame is in your environment and pandas was
imported as pd.

1 - Print the header of account_opened from the banking DataFrame and take a look at the different results.
2 - Convert the account_opened column to datetime, while making sure the date format is inferred and that erroneous formats that raise error return a missing value.
3 - Extract the year from the amended account_opened column and assign it to the acct_year column.
4 - Print the newly created acct_year column.

    # Print the header of account_opend
    print(banking['account_opened'].head())

    # Convert account_opened to datetime
    banking['account_opened'] = pd.to_datetime(banking['account_opened'],
    # Infer datetime format
    infer_datetime_format = True,
    # Return missing value for error
    errors = 'coerce')

    # Get year of account opened
    banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')

    # Print acct_year
    print(banking['acct_year'])

    -> pd.to_datetime을 통해 변경할떄, 인수들을 잘 기억해두자.
    infer_datetime_format -> 추측을 통해 데이터타입을 변경할 것이라면 True
    범위를 벗어나거나 오류에 대해 NaT을 반환하려면 errors = 'coerce'

    datetime 형식을 변경하려면, dt.strftime('')을 통해 변경경

---------------------------------------------------------------------

Cross field validation

    - 지저분한 값들을 진단하기 위한 교차필드 유효성 검사

    - 서로 다른 데이터를 병합하는 것이 데이터의 무결성을 위배하지 않는지, 데이터가 올바른지
    확인하기 위해 사용

    -> 열 혹은 행에 대한 계산을 통해 특정 값이 유효한 값인지 확인.

    .sum(axis=1) -> 행별로의 연산을한다. 열별로 하고 싶을때는 axis=0

----------------------------------------------------------------------

New data has been merged into the banking DataFrame that contains details on how
investments in the inv_amount column are allocated across four different funds A, B, C
and D.

Furthermore, the age and birthdays of customers are now stored in the age and birth_date
columns respectively.

You want to understand how customers of different age groups invest. However, you want
to first make sure the data you're analyzing is correct. You will do so by cross field
checking values of inv_amount and age against the amount invested in different funds
and customers' birthdays. Both pandas and datetime have been imported as pd and dt
respectively.

1 - Find the rows where the sum of all rows of the fund_columns in banking are equal to the inv_amount column.
2 - Store the values of banking with consistent inv_amount in consistent_inv, and those with inconsistent ones in inconsistent_inv.

    # Store fund columns to sum against
    fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']

    # Find rows where fund_columns row sum == inv_amount
    inv_equ = banking[fund_columns].sum(axis=1) == banking['inv_amount']

    # Store consistent and inconsistent data
    consistent_inv = banking[inv_equ]
    inconsistent_inv = banking[~inv_equ]

    # Store consistent and inconsistent data
    print("Number of inconsistent investments: ", inconsistent_inv.shape[0])

    -> 합계를 더할 컬럼들을 미리 리스트를 통해 생성해 둔다.
    그런 후 sum(axis=1)을 통해 행에 대한 합계를 계산하고, banking의 inv_amount 값과 일치하는지를
    불리언의 형태로 inv_equ에 저장한다.

    그리고 consistent에는 해당 조건을 만족하는 데이터프레임을,
    inconsistent에는 조건을 만족하지 않는 데이터 프레임을 생성하였다.

    마지막으로 만족하지 않는 값들의 갯수는 .shape[0]을 통해 얻을 수 있다.
    .shape[0]은 행의 갯수를, .shape[1]은 열의 갯수를 반환한다.

1 - Store today's date into today, and manually calculate customers' ages and store them in ages_manual.
2 - Find all rows of banking where the age column is equal to ages_manual and then filter banking into consistent_ages and inconsistent_ages.

    # Store today's date and find ages
    today = dt.date.today()
    ages_manual = today.year - pd.to_datetime(banking['birth_date']).dt.year

    # Find rows where age column == ages_manual
    age_equ = banking['age'] == ages_manual

    # Store consistent and inconsistent data
    consistent_ages = banking[age_equ]
    inconsistent_ages = banking[~age_equ]

    # Store consistent and inconsistent data
    print("Number of inconsistent ages: ", inconsistent_ages.shape[0])

    -> 일반적으로 .dt 를 사용하는 경우, 데이터 타입이 datetime이 아닌경우에는 따로
    pd.to_datetime을 통해 변경을 해준 후 , 사용해야 한다.
    오늘의 날짜 데이터를 받기 위해서는 항상 dt.date.today()를 사용한다.
    ()를 사용하지 않아 오류가 여러번 반복되었다.

----------------------------------------------------------------------

Missing data

    NA, ana, 0, ., ...

    -> isna()를 통해 결측값의 여부를 불리언의 형태로 얻을 수 있다.
    누락값은 True, 아닌 값을 False로 반환한다.

    isna().sum()을 실행하면 해당 컬럼에 대해서 몇개의 누락값이 존재하는지 반환한다.


***** Missingno 패키지를 사용하여 누락된 데이터들에 대한 유용한 시각화 자료를 만들 수 있다.

    ex)
        import missingno as msno
        import matplotlib.pyplot as plt

        msno.matrix(airquality)
        plt.show()

        missing = airquality[airquality['CO2'].isna()]
        complete = airquality[~airquality['CO2'].isna()]

        -> 누락값들이 존재하는 데이터 프레임을 따로 생성하여 분리하였다.

    Missing types

        Missing completely at random(MCAR)

            임의의 데이터에서 완전한 누락은 임의성으로 인해 데이터가 완전히 누락된 경우.
            누락된 데이터와 나머지 값들 사이에 전혀 관계가 없다.
            주로 입력시의 오류로 발생.

        Missing at Random(MAR)

            '다른 측정치'(측정된)와 결측치 사이의 시스템적인 관계가 있는 경우.
            어떤 원인에 의해 결측치가 발생한 경우.

        Missing not at random(MNAR)

            '측정되지 않은' 어떤 값들과 결측치 사이에 시스템적인 관계가 있는 경우

            -> 무엇인지 실제로는 알 수 없다.

    How to deal with missing data?

        simple approaches - drop, impute with statistical measures(mean, median ,...)
        complex approaches - algorithmic approach, machine learning models


        ex) Replacing

            co2_mean = airquality['CO2'].mean() -> co2값의 평균을 co2_mean에 저장해둔다.

            airquality-imputed = airquality.fillna({'CO2':co2_mean})


            -> airquality의 CO2컬럼에 NaN값이 존재한다면 co2_mean으로 채운다.

----------------------------------------------------------------------

Dealing with missing data is one of the most common tasks in data science. There are a
variety of types of missingness, as well as a variety of types of solutions to missing
data.

You just received a new version of the banking DataFrame containing data on the amount
held and invested for new and existing customers. However, there are rows with missing
inv_amount values.

You know for a fact that most customers below 25 do not have investment accounts yet,
and suspect it could be driving the missingness. The pandas, missingno and
matplotlib.pyplot packages have been imported as pd, msno and plt respectively. The
banking DataFrame is in your environment.

1 - Print the number of missing values by column in the banking DataFrame.
2 - Plot and show the missingness matrix of banking with the msno.matrix() function.
3 - Isolate the values of banking missing values of inv_amount into missing_investors and with non-missing inv_amount values into investors.
4 - Sort the banking DataFrame by the age column and plot the missingness matrix of banking_sorted.

    # Print number of missing values in banking
    print(banking.isna().sum())

    # Visualize missingness matrix
    msno.matrix(banking)
    plt.show()

    # Isolate missing and non missing values of inv_amount
    missing_investors = banking[banking['inv_amount'].isna()]
    investors = banking[~banking['inv_amount'].isna()]

    # Sort banking by age and visualize
    banking_sorted = banking.sort_values(by='age')
    msno.matrix(banking_sorted)
    plt.show()

----------------------------------------------------------------------

In this exercise, you're working with another version of the banking DataFrame that
contains missing values for both the cust_id column and the acct_amount column.

You want to produce analysis on how many unique customers the bank has, the average
amount held by customers and more. You know that rows with missing cust_id don't really
help you, and that on average acct_amount is usually 5 times the amount of inv_amount.

In this exercise, you will drop rows of banking with missing cust_ids, and impute
missing values of acct_amount with some domain knowledge.

1 - Use .dropna() to drop missing values of the cust_id column in banking and store the results in banking_fullid.
2 - Compute the estimated acct_amount of banking_fullid knowing that acct_amount is usually inv_amount * 5 and assign the results to acct_imp.
3 - Impute the missing values of acct_amount in banking_fullid with the newly created acct_imp using .fillna().

    # Drop missing values of cust_id
    banking_fullid = banking.dropna(subset = ['cust_id'])

    # Compute estimated acct_amount
    acct_imp = banking_fullid['inv_amount'] * 5

    # Impute missing acct_amount with corresponding acct_imp
    banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})

    # Print number of missing values
    print(banking_imputed.isna().sum())

    -> 먼저 cust_id 컬럼상에 결측치가 존재한다면 해당 행을 제거한다.
    그런 뒤 banking_fullid 데이터 프레임상에 결측치를 다시 조사해보면 그대로 14개가 나온다.
    이 값들을 처리하기 위해, 일반적인 acct_amount값을 acct_imp에 저장을 해 둔다.
    그리고 fillna를 통해 결측치를 미리 계산해둔 acct_imp값으로 대체한다.

----------------------------------------------------------------------

Minimum edit distance

    -> 두개의 문자열이 얼마나 가까운지를 식별하는 체계

    - 더하기, 빼기, 자리 바꾸기, 대체하기의 4가시 작업으로 이루어져 있다.

    ex) INTENTION 과 EXECUTION 두 단어에 대해서 보면,
        첫번쨰 작업으로, I를 제거하고 E와N 사이에 C를 추가한다

        그러면 두 문자열은 다음과 같아진다.

        I   N   T   E   *   N   T   I   O   N
        *   E   X   E   C   U   T   I   O   N

        -> 3개의 작업을 추가적으로 한다면, 두 문자열이 일치하게 된다.
        총 2+5 = 5의 편집거리를 가지게 된다.


    * Levenshtein distance를 사용하여 문자열을 비교한다.
    fuzzywuzzy 패키지의 가장 일반적인 형태의 문자열 매칭이다.

        from fuzzywuzzy import fuzz

        fuzz.WRatio('Reeding', 'Reading')

        -> 86
        비교결과 점수를 반환하며, 100이면 일치, 0이면 전혀 불일치이다.

    - 순서가 다른 문자열 비교시에 매우 유용하다.

        fuzz.WRatio('Houston Rockets', 'Rockets')

        -> 90

        fuzz.WRatio(Houston Rockets vs Los Angeles Lakers', 'Lakers vs Rockets')

        -> 86

        process.extract(string, choices, limit=2)

        -> 3개의 요소를 가지고 있는 튜플 목록을 반환.
        첫번째는 일치하는 문자열, 두번째는 유사성 점수, 마지막은 배열의 인덱스이다.


    - 2장에서 배운 카테고리를 합치는 과정에서, 카테고리가 너무 다양하다면 어떻게 바꿀것인가?

        ex) EU, eur, Europ, Europa, Erope, Evropa, ... 를 모두 replace를 통해 Europe로 바꾸고 싶다면

        -> String similarity를 사용한다.
        (주로 프리텍스트 형식인 경우 많이 발생한다.)

        루프를 통해 Collapse 해보자

        for state in categories['state'] :

            matches = process.extract(state, survey['state'], limit = survey.shape[0])

            for potential_match in matches :

                if potential_match[1] >= 80 :

                    survey.loc[survey['state']==potential_match[0], 'state'] = state

----------------------------------------------------------------------

In this exercise, and throughout this chapter, you'll be working with the restaurants
DataFrame which has data on various restaurants. Your ultimate goal is to create a
restaurant recommendation engine, but you need to first clean your data.

This version of restaurants has been collected from many sources, where the
cuisine_type column is riddled with typos, and should contain only italian, american
and asian cuisine types. There are so many unique categories that remapping them
manually isn't scalable, and it's best to use string similarity instead.

Before doing so, you want to establish the cutoff point for the similarity score
using the fuzzywuzzy's process.extract() function by finding the similarity score of
the most distant typo of each category.

1 - Import process from fuzzywuzzy.
2 - Store the unique cuisine_types into unique_types.
3 - Calculate the similarity of 'asian', 'american', and 'italian' to all possible cuisine_types using process.extract(), while returning all possible matches.

    # Import process from fuzzywuzzy
    from fuzzywuzzy import process

    # Store the unique values of cuisine_type in unique_types
    unique_types = restaurants['cuisine_type'].unique()

    # Calculate similarity of 'asian' to all values of unique_types
    print(process.extract('asian', unique_types, limit = len(unique_types)))

    # Calculate similarity of 'american' to all values of unique_types
    print(process.extract('american', unique_types, limit = len(unique_types)))


    # Calculate similarity of 'italian' to all values of unique_types
    print(process.extract('italian', unique_types, limit = len(unique_types)))

    -> fuzzywuzzy의 process를 사용하기 위해 fuzzywuzzy에서 process를 import한다.
    레스토랑 데이터 프레임에서 cuisine_type행의 유일한 값들을 리스트의 형태로 저장한다.

    각각의 단어들('asian', 'american', 'italian') 별로의 리스트 내의 이름들과
    유사한 정도를 점수로서 받는다.

    -> 약 80점 이상은 되어야 원래의 단어와 매우 유사한 형태를 이룬다.

----------------------------------------------------------------------

In the last exercise, you determined that the distance cutoff point for remapping
typos of 'american', 'asian', and 'italian' cuisine types stored in the cuisine_type
column should be 80.

In this exercise, you're going to put it all together by finding matches with
similarity scores equal to or higher than 80 by using fuzywuzzy.process's extract()
function, for each correct cuisine type, and replacing these matches with it. Remember,
when comparing a string with an array of strings using process.extract(), the output
is a list of tuples where each of tuple is as such:

(closest match, similarity score, index of match)
The restaurants DataFrame is in your environment, and you have access to a categories
list containing the correct cuisine types.

1 - Iterate over each cuisine in the categories list.
2 - For each cuisine, find its similarity to entries in the cuisine_type column of restaurants, while returning all possible matches and store them in matches.
3 - For each possible match in matches equal or higher than 80, find the rows where the cuisine_type in restaurants is equal to that possible match. Assign this to matching_cuisine.
4 - Use the Boolean series matching_cuisine to filter restaurants and replace that match with the correct cuisine.
5 - Print the new unique values of cuisine_type in restaurants.

    categories = ['asian', 'american', 'italian']

    # For each correct cuisine_type in categories
    for cuisine in categories:
        # Find matches in cuisine_type of restaurants
        matches = process.extract(cuisine, restaurants['cuisine_type'], limit = restaurants.shape[0])

        # For each possible_match with similarity score >= 80
        for possible_match in matches:
            if possible_match[1] >= 80:
                # Find matching cuisine type
                matching_cuisine = restaurants['cuisine_type'] == possible_match[0]
                restaurants.loc[matching_cuisine, 'cuisine_type'] = cuisine

    # Print unique values to confirm mapping
    print(restaurants['cuisine_type'].unique())

    -> 먼저 통일시킬 카테고리를 리스트에 저장을 한다.

    1) 카테고리 내의 항목들에 대해서 반복문을 시행한다.

        각 항목들에 대해서 레스토랑 데이터프레임의 'cuisine_type' 컬럼 값들과 비교를 해서
        얼마나 일치하는지 값을 추출한다.

        -> matches에는 각 조합들에 대한 일치도가 리스트의 형태로 저장된다.

        2) 각 조합들에 대해서 두번째 반복문을 시행한다.

            만약 각 일치도가 80 이상이라면, (possible_match는 튜플형으로, [0]은 인덱스, [1]은 일치도 값)
            cuisine_type값이 possible_match[0]의 값이 같은지를 불리언의 형태로
            matching_cuisine에 저장한다.

            그리고, 레스토랑 데이터프레임의 cuisine_type 컬럼 값을 변경해준다.
            카테고리 내의 값과 일치한다면 그대로.(불리언이 False일 것이기 때문에.)
            값과 일치하지 않는다면 cuisine값을 대입하여 변경한다.

----------------------------------------------------------------------

Generating pairs

    - 목표 : 서로 다른 두 데이터프레임 상에서 연관있는 값들을 연결시키는 것이 최종 목표

    * Record linkage

        중복을 피하며 병합을 하고 싶다!

        1) 두 데이터 프레임 사이에 가능한 모든 쌍을 생성한다. -> 매우 불합리적

        2) Blocking 일치하는 열을 사용하여 가능한 쌍의 수를 줄인다.

            import recordlinkage()

            indexer = recordlinkage.Index()
            -> indexing object의 생성. 데이터 프레임 내에서 쌍을 생성할수 있는 객체

            indexer.block('state')
            pairs = indexer.index(census_A, census_B)
            -> block 매서드를 사용하여 쌍을 생성.(차단 방법을 사용)

            pairs는 두 데이터프레임의 행 인덱스 쌍을 포함하는 pandas 다중 인덱스 객체

            compare_cl = recordlinkage.Compare()
            -> 비교 객체를 생성.
            쌍을 생성하는 동안, 쌍에 대한 비교절차를 할당하는 역할

            만약 쌍 간에 정확히 일치하는 열이 존재한다면,
            compare_cl.exact('date_of_birth', 'date_of_birth', label='date_of_birth')
            compare_cl.exact('state','state',label='state')
            -> 각 데이터프레임의 열 이름을 받는다.

            정확히 일치하는 것이 아니라 string 유사도가 기준치 이상보다 높아야 하는 경우
            compare_cl.string('surname', 'surname', threshold=0.85, label='surname'))

            이제 최종적인 매치를 하기 위해
            potential_matches = compare_cl.compute(pairs, census_A, census_B)

    ***** 주의할 점 : 두 데이터프레임은 동일한 순서를 가지고 있어야 한다.

            -> 이 결과 얻는 출력은 다중 인덱스 데이터프레임

                첫번째 인덱스는 데이터프레임1의 인덱스, 두번째는 데이터프레임2의 인덱스

                열은 비교되는 열이며, 일치하는 경우에는 1, 일치하지 않으면 0의 값을 갖는다.

            행에 대한 일치도의 합이 2 이상인 경우만 찾고 싶다면,

                potential_matches[potential_matches.sum(axis=1) => 2]

--------------------------------------------------------------------

In the last lesson, you cleaned the restaurants dataset to make it ready for
building a restaurants recommendation engine. You have a new DataFrame named
restaurants_new with new restaurants to train your model on, that's been scraped
from a new data source.

You've already cleaned the cuisine_type and city columns using the techniques
learned throughout the course. However you saw duplicates with typos in
restaurants names that require record linkage instead of joins with restaurants.

In this exercise, you will perform the first step in record linkage and generate
possible pairs of rows between restaurants and restaurants_new. Both DataFrames,
pandas and recordlinkage are in your environment.

1 - Instantiate an indexing object by using the Index() function from recordlinkage.
2 - Block your pairing on cuisine_type by using indexer's' .block() method.
3 - Generate pairs by indexing restaurants and restaurants_new in that order.

    # Create an indexer and object and find possible pairs
    indexer = recordlinkage.Index()
    -> 인덱싱 객체를 생성하는 코드. indexer가 인덱싱 객체이다.

    # Block pairing on cuisine_type
    indexer.block('cuisine_type')

    -> recordlinkage.Index().block() 은 해당 컬럼에 대해서 가능한 연결로 pair을 생성한다.
    만약 full()을 한다면 전체에 대해서 모든 pair를 생성한다.

    # Generate pairs
    pairs = indexer.index(restaurants, restaurants_new)

    -> 두 데이터프레임을 변수로 받아 인덱싱을 실행한다.

----------------------------------------------------------------------

In the last exercise, you generated pairs between restaurants and restaurants_new
in an effort to cleanly merge both DataFrames using record linkage.

When performing record linkage, there are different types of matching you can
perform between different columns of your DataFrames, including exact matches,
string similarities, and more.

Now that your pairs have been generated and stored in pairs, you will find exact
matches in the city and cuisine_type columns between each pair, and similar
strings for each pair in the rest_name column. Both DataFrames, pandas and
recordlinkage are in your environment.

1 - Instantiate a comparison object using the recordlinkage.Compare() function.
2 - Use the appropriate comp_cl method to find exact matches between the city and cuisine_type columns of both DataFrames.
3 - Use the appropriate comp_cl method to find similar strings with a 0.8 similarity threshold in the rest_name column of both DataFrames.
4 - Compute the comparison of the pairs by using the .compute() method of comp_cl.

    # Create a comparison object
    comp_cl = recordlinkage.Compare()

    # Find exact matches on city, cuisine_types -
    comp_cl.exact('city', 'city', label='city')
    comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')

    # Find similar matches of rest_name
    comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8)

    # Get potential matches and print
    potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)
    print(potential_matches)

    -> 먼저 비교객체 comp_cl을 생성한다
    정확히 일치하는 대상을 찾는 비교는 comp_cl.exact를 사용하고,
    유사도가 있는 경우, 그 값이 일정 기준 이상의 대상을 찾는 비교는 .string을 사용한다.
    .string같은 경우 기준치가 있어야 하며 threshold = 을 사용해 설정한다.

    마지막으로 compute를 사용하여 두 데이터프레임 간의 쌍을 찾는다.
    (각 record pair 별로.)

----------------------------------------------------------------------

Record linkage

    Generate pairs -> Compare pairs -> Score pairs -> Link data

    앞서 생성한 potential_matches중에서 행별 점수 합이 기준치 이상인 값들을 선정

    matches = potential_matches[potential_matches.sum(axis=1) >= 3]

    -> 중복되는, 두 데이터프레임상에서 일치하는 데이터들을 따로 저장한다.
    (인덱스들을 저장한다.)

    duplicate_rows = matches.index.get_level_values(1)

    기존의 데이터 프레임 상에서, 이 인덱스들이 포함된 행들을 통해 새로운 데이터 프레임을 생성한다.

    census_B_new = census_B[~census_B.index.isin(duplicate_rows)]

    -> 위의 겹치는 중복 행들을 제외한 데이터프레임을 생성한다.

    full_census = census_1.append(census_B_new)

----------------------------------------------------------------------

In the last lesson, you've finished the bulk of the work on your effort to link
restaurants and restaurants_new. You've generated the different pairs of
potentially matching rows, searched for exact matches between the cuisine_type
and city columns, but compared for similar strings in the rest_name column. You
stored the DataFrame containing the scores in potential_matches.

Now it's finally time to link both DataFrames. You will do so by first extracting
all row indices of restaurants_new that are matching across the columns mentioned
above from potential_matches. Then you will subset restaurants_new on these
indices, then append the non-duplicate values to restaurants. All DataFrames are
in your environment, alongside pandas imported as pd.

1 - Isolate instances of potential_matches where the row sum is above or equal to 3 by using the .sum() method.
2 - Extract the second column index from matches, which represents row indices of matching record from restaurants_new by using the .get_level_values() method.
3 - Subset restaurants_new for rows that are not in matching_indices.
4 - Append non_dup to restaurants.

    # Isolate potential matches with row sum >=3
    matches = potential_matches[potential_matches.sum(axis=1) >= 3]

    # Get values of second column index of matches
    matching_indices = matches.index.get_level_values(1)

    # Subset restaurants_new based on non-duplicate values
    non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]

    # Append non_dup to restaurants
    full_restaurants = restaurants.append(non_dup)
    print(full_restaurants)

    -> potential_matches는 comp_cl을 통해 생성한 가능성이 존재하는 모든 pair들의
    데이터 프레임이다.
    이중에 행별 점수를 매겨, 기준치 이상. 이 경우에는 3점 이상인 경우 일치하는 행으로 판단

    그 값들의 데이터 프레임을 matches에 저장한다.

    그리고 겹치는 행들의 인덱스를 두번째 데이터프레임의 인덱스에서 추출하여
    matching_indices에 저장한다.

    두번째 데이터프레임에서의 인덱스들 중, 이 중복값들의 인덱스 내에 없는 값들로만
    겹침을 제거한 새로운 데이터 프레임을 생성한다. non_dup

    그리고 첫번째 데이터 프레임에 이 중복을 제거한 non_dup 데이터 프레임을 append 한다.

------------------------------------------------