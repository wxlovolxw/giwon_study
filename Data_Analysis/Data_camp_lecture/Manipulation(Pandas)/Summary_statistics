Summary Statistics

    .mean(), median()
    .mode()
    .min(), .max()
    .var(), .std()
    .sum()
    .quantile() - 분위수

        -> ex)  def pct30(column) :
                    return column.quantile(0.3)

                -> DataFrame 열의 30번때 백분위수를 계산하는 함수

    .cumsum() - 누적합계의 계산. cumulative sum
    .cummin(), cumprod() - 누적최소값, 누적곱

        df["column"].cumsum()

    -> 여러행에 대해서 시항할 때에는

        df[["column_1",column_2]].agg(function)

    -> 여러 함수를 적용시킬 대에는

        df["column"].agg([func_1,func_2])

//////////////////////////////////////////////////////////////////

Explore your new DataFrame first by printing the first few rows of the sales DataFrame.
Print information about the columns in sales.
Print the mean of the weekly_sales column.
Print the median of the weekly_sales column.

    # Print the head of the sales DataFrame
    print(sales.head())

    # Print the info about the sales DataFrame
    print(sales.info())
    # Print the mean of weekly_sales
    print(sales.weekly_sales.mean())

    # Print the median of weekly_sales
    print(sales["weekly_sales"].median())

    - 컬럼을 선택할 때에, 두가지 방식이 있다.

        df.column_name
        df["column_name"]

        -> 두방식에서 차이는 없지만 전자에서는 열 이름이 숫자로 시작하지 않고,
        공백 및 특수문자가 없다는 조건을 만족해야 한다. 간결성 측면에서는 더 좋지만
        후자를 이용하는 방식에 익숙해져야 할 것 같다.

//////////////////////////////////////////////////////////////////

Print the maximum of the date column.
Print the minimum of the date column.

    # Print the maximum of the date column
    print(sales.date.max())

    # Print the minimum of the date column
    print(sales.date.min())

    -> 조건문이 없을 때에는 .을 써서 컬럼을 호출하는 것이 더 간결해 보인다.

//////////////////////////////////////////////////////////////////

Use the custom iqr function defined for you along with .agg() to print the IQR of the temperature_c column of sales.

    agg() - aggregate, DataFrame의 컬럼을 기준으로 통계량을 집계해 준다.
            pandas와 numpy에서 제공하는 함수 외에도 사용자 정의 함수를 사용 할 수 있다.

            -> apply는 컬럼을 기준으로 함수를 적용하기 위해 사용하기 때문에
            agg는 apply의 특수한 경우라 생각하면 된다. 집계에서는 agg가 성능이 빠르다.

    # A custom IQR function
    def iqr(column):
    return column.quantile(0.75) - column.quantile(0.25)

    # Print IQR of the temperature_c column
    print(sales["temperature_c"].agg(iqr))

Update the column selection to use the custom iqr function with .agg() to print the IQR of temperature_c, fuel_price_usd_per_l, and unemployment, in that order.

    # A custom IQR function
    def iqr(column):
        return column.quantile(0.75) - column.quantile(0.25)

    # Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment
    print(sales[["temperature_c", "fuel_price_usd_per_l", "unemployment"]].agg(iqr))

Update the aggregation functions called by .agg(): include iqr and np.median in that order.

    # Import NumPy and create custom IQR function
    import numpy as np
    def iqr(column):
        return column.quantile(0.75) - column.quantile(0.25)

    # Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment
    print(sales[["temperature_c", "fuel_price_usd_per_l", "unemployment"]].agg([iqr,np.median]))

//////////////////////////////////////////////////////////////////

Sort the rows of sales_1_1 by the date column in ascending order.
Get the cumulative sum of weekly_sales and add it as a new column of sales_1_1 called cum_weekly_sales.
Get the cumulative maximum of weekly_sales, and add it as a column called cum_max_sales.
Print the date, weekly_sales, cum_weekly_sales, and cum_max_sales columns.

    # Sort sales_1_1 by date
    sales_1_1 = sales_1_1.sort_values("date", ascending=True)

    # Get the cumulative sum of weekly_sales, add as cum_weekly_sales col
    sales_1_1["cum_weekly_sales"] = sales_1_1.weekly_sales.cumsum()

    # Get the cumulative max of weekly_sales, add as cum_max_sales col
    sales_1_1["cum_max_sales"] = sales_1_1.weekly_sales.cummax()

    # See the columns you calculated
    print(sales_1_1[["date", "weekly_sales", "cum_weekly_sales", "cum_max_sales"]])

//////////////////////////////////////////////////////////////////

Counting

    범주형 데이터의 요약시에 사용.

    - .drop_duplicates(subset="column") : 데이터 셋에서 중복된 이름을 한번만 추출
    -> 두가지 이상의 컬럼에 대해서 실행한다면,
        subset=["column_1","column_2"]

    - .value_counts() : 해당 열의 하위집합을 만든 뒤 각각의 값들이 몇개씩 있는지 카운트 한다.
        df["column"].value_counts()
        -> sort=True를 통해 오름차순으로 정렬 할 수 있다.
        -> ascending=False를 통해 내림차순으로 정렬 할 수 있다.
        -> normalize=True를 통해 몇분위를 차지하는지 알 수 있다.

/////////////////////////////////////////////////////////////////

Remove rows of sales with duplicate pairs of store and type and save as store_types and print the head.
Remove rows of sales with duplicate pairs of store and department and save as store_depts and print the head.
Subset the rows that are holiday weeks, and drop the duplicate dates, saving as holiday_dates.
Select the date column of holiday_dates, and print.

    # Drop duplicate store/type combinations
    store_types = sales.drop_duplicates(subset=["store","type"])
    print(store_types.head())

    # Drop duplicate store/department combinations
    store_depts = sales.drop_duplicates(subset=["store","department"])
    print(store_depts.head())

    # Subset the rows that are holiday weeks and drop duplicate dates
    holiday_dates = sales[sales["is_holiday"]==True].drop_duplicates(subset="date")

    # Print date col of holiday_dates
    print(holiday_dates.date)


//////////////////////////////////////////////////////////////////

Count the number of stores of each store type in store_types.
Count the proportion of stores of each store type in store_types.
Count the number of different departments in store_depts, sorting the counts in descending order.
Count the proportion of different departments in store_depts, sorting the proportions in descending order.

    # Count the number of stores of each type
    store_counts = store_types["type"].value_counts()
    print(store_counts)

    # Get the proportion of stores of each type
    store_props = store_types["type"].value_counts(normalize=True)
    print(store_props)

    # Count the number of each department number and sort
    dept_counts_sorted = store_depts["department"].value_counts(ascending=False)
    print(dept_counts_sorted)

    # Get the proportion of departments of each number and sort
    dept_props_sorted = store_depts["department"].value_counts(ascending=False, normalize=True)
    print(dept_props_sorted)

//////////////////////////////////////////////////////////////////

Grouped summary statistics

    - 다른 그룹과 비교하는 데에 매우 유용하다.
    - 개별 그룹의 요약을 통해 통찰력을 얻을 수 있다.

    ex) 어떤 색의 개들이 평균무게가 가장 큰지
        암컷 개가 수컷 개보다 큰지

        -> subset을 통해 각각의 요약통계값을 얻을 수 있다.
        -> 이때 복제되어 반복된 코드는 버그를 유발할 수 있다.

    -> groupby 메서드를 사용한다.

        dogs.groupby("color")["weight_kg"].mean()
        .agg([min,max,sum]) 을 통해 여러 통계치를 얻을 수 있다.
        groupby(["color","breed"])["weight"] 를 통해 색상과 품종별로 그룹을 나눈다.

//////////////////////////////////////////////////////////////////

Calculate the total weekly sales over the whole dataset.
Subset for type "A" stores, and calculate their total weekly sales.
Do the same for type "B" and type "C" stores.
Combine the A/B/C results into a list, and divide by overall sales to get the proportion of sales by type.

    # Calc total weekly sales
    sales_all = sales["weekly_sales"].sum()

    # Subset for type A stores, calc total weekly sales
    sales_A = sales[sales["type"] == "A"]["weekly_sales"].sum()

    # Subset for type B stores, calc total weekly sales
    sales_B = sales[sales["type"] == "B"]["weekly_sales"].sum()

    # Subset for type C stores, calc total weekly sales
    sales_C = sales[sales["type"] == "C"]["weekly_sales"].sum()

    # Get proportion for each type
    sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all

    print(sales_propn_by_type)

    -> groupby를 사용하지 않고 각 타입별 통계치를 얻는 방법.
    매우 비효율 적이다.

//////////////////////////////////////////////////////////////////

Group sales by "type", take the sum of "weekly_sales", and store as sales_by_type.
Calculate the proportion of sales at each store type by dividing by the sum of sales_by_type. Assign to sales_propn_by_type.

    # Group by type; calc total weekly sales
    sales_by_type = sales.groupby("type")["weekly_sales"].sum()

    # Get proportion for each type
    sales_propn_by_type = sales_by_type / sales_by_type.sum()
    print(sales_propn_by_type)

Group sales by "type" and "is_holiday", take the sum of weekly_sales, and store as sales_by_holiday_type.

    # From previous step
    sales_by_type = sales.groupby("type")["weekly_sales"].sum()

    # Group by type and is_holiday; calc total weekly sales
    sales_by_type_is_holiday = sales.groupby(["type","is_holiday"])["weekly_sales"].sum()
    print(sales_by_type_is_holiday)

//////////////////////////////////////////////////////////////////

Import NumPy with the alias np.
Get the min, max, mean, and median of weekly_sales for each store type using .groupby() and .agg(). Store this as sales_stats. Make sure to use numpy functions!
Get the min, max, mean, and median of unemployment and fuel_price_usd_per_l for each store type. Store this as unemp_fuel_stats.

    # Import NumPy with the alias np
    import numpy as np

    # For each store type, aggregate weekly_sales: get min, max, mean, and median
    sales_stats = sales.groupby("type")["weekly_sales"].agg([np.min,np.max,np.mean,np.median])

    # Print sales_stats
    print(sales_stats)

    # For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median
    unemp_fuel_stats = sales.groupby("type")["unemployment","fuel_price_usd_per_l"].agg([np.min,np.max,np.mean,np.median])

    # Print unemp_fuel_stats
    print(unemp_fuel_stats)

//////////////////////////////////////////////////////////////////

Pivot tables

    - 그룹화 된 요약 통계를 계산하는 방법중의 하나.

    Pandas에서 pivot table만드는 방법.

        기존의 방식 : dogs.groupby("color")["weight_kg"].mean()
        을 통해 색상별로 그룹화 하여 평균 무게를 계산함.

        dogs.pivot_table(value="weight_kg",index="color")
        -> 평균값에 대해 알려준다.
        -> 다른 함수를 사용하고 싶다면 aggfunc=np.func
        -> 여러 함수라면 aggfunc=[np.mean, np.median]

        -> 두개의 변수로 그룹화 하기 위해 두번쨰 변수의 이름을 column 인수에 전달
        values="weight_kg", index="color", columns="breed"

        -> fill_value = value 를 통해 NaN값을 채울 수 있다.

        -> margins=True를 통해 결측치를 포함하지 않은 상태에서 모든값의 평균을 구할 수 있다.

//////////////////////////////////////////////////////////////////

Get the mean weekly_sales by type using .pivot_table() and store as mean_sales_by_type.

    # Pivot for mean weekly_sales for each store type
    mean_sales_by_type = sales.pivot_table(values="weekly_sales", index="type")

    # Print mean_sales_by_type
    print(mean_sales_by_type)

Get the mean and median (using NumPy functions) of weekly_sales by type using .pivot_table() and store as mean_med_sales_by_type.

    # Import NumPy as np
    import numpy as np

    # Pivot for mean and median weekly_sales for each store type
    mean_med_sales_by_type = sales.pivot_table(values="weekly_sales", index="type", aggfunc=[np.mean, np.median])

    # Print mean_med_sales_by_type
    print(mean_med_sales_by_type)

Get the mean of weekly_sales by type and is_holiday using .pivot_table() and store as mean_sales_by_type_holiday.

    # Pivot for mean weekly_sales by store type and holiday
    mean_sales_by_type_holiday = sales.pivot_table(values="weekly_sales", index="type", columns="is_holiday")

    # Print mean_sales_by_type_holiday
    print(mean_sales_by_type_holiday)

//////////////////////////////////////////////////////////////////

Print the mean weekly_sales by department and type, filling in any missing values with 0.

    # Print mean weekly_sales by department and type; fill missing values with 0
    print(sales.pivot_table(values="weekly_sales", index="department", columns="type", fill_value=0))

Print the mean weekly_sales by department and type, filling in any missing values with 0 and summing all rows and columns.

    # Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols
    print(sales.pivot_table(values="weekly_sales", index="department", columns="type", fill_value=0, margins=True))

