
분류(Classification) 문제

    - 독립변수값이 주어졌을 때, 가장 연관성이 큰 종속 변수의 값을 예측한다.

    - 선택해야 할 카테고리 혹은 class가 미리 주어진다.

    -> 어떤 표본에 대한 데이터가 주어졌을 때, 해당 표본이 어떤 카테고리 혹은 클래스에 속하는지 예측해야 함.

//////////////////////////////////////////////////////////////////////////////////////////////////////////

단일모델

    확률적 모형 : 주어진 데이터에 대해 각 카테고리 혹은 클래스가 정답일 조건부 확률을 계산하는 모형

        - 확률적 판별 모형(discriminative) : 직접 조건부 확률 함수의 모양을 추정하는 모형

            ex) 로지스틱회귀, 의사결정나무

        - 확률적 생성 모형(generative) : 베이즈 이론을 통해 간접적인 조건부 확률을 구하는 모형

            ex) 나이브 베이지안, LDA/QDA

    판별 함수 모형 : 주어진 데이터를 카테고리에 따라 서로 다른 영역으로 나누는 경계면을 찾고,
                    해당 데이터가 어느 위치에 있는지를 계산하는 판별함수를 이용하는 모형

            ex) 퍼셉트론, 서포트벡터머신, 인공신경망망

    ---------------------------------------------------------------------------------------------------

    1) 나이브 베이즈 분류기(Naive bayes classifier)

        - 데이터의 각 특성을 개별로 취급하여 매개변수를 학습시키고, 각 특성에서 클래스 별로 통계를 취합한다.

        - 선형 모델과 매우 유사하나 로지스틱 회귀나 선형 SCV보다 훈련 속도가 빠르고, 일반화 성능은 뒤쳐진다.

        - 각 특성이 독립적이며 동등하게 중요하다는 가정이 들어간다.

        - 종류 : 가우시안 NB, 베르누이 NB, 다항분포 NB

            가우시안 NB - 연속적인 데이터에 적용. 대부분 매우 고차원인 데이터세트에 사용된다.
                        클래스별로 각 특성의 평균과 표준편차를 저장하고 이를 데이터 포인트와 비교해 예측값을 출력

            다항분포 NB - 카운트 데이터(문장속에 사용된 단어의 횟수)에 적용.
                        클래스별로 특성의 평균을 계산. 예측 공식이 선형모델과 형태가 같다.

            베르누이 NB - 주로 이진 데이터에 적용.

    ---------------------------------------------------------------------------------------------------

    2) 서포트 벡터 머신(SVC)

        - 범주를 두개로 나누는 분류 문제에 대해서 더 나은 분류 경계면을 구하는 방법.

        - 두 범주의 평면에 대해서 그 거리(마진-margin)를 최대로 만들어야 한다.

        - 서포트 벡터(SV)는 마진을 결정짓는, 구분선을 긋기 위해 선탠된 점.

        - kernel의 종류는 선형, 다항, RBF(Radial Basis Function), 가우시안, 시그모이드 등이 있다.

        - 선형은 차원을 확장하지 않고 판별을 하기 때문에 XOR 문제등을 풀수 없다.(카테고리형)

        - D-차원의 독립변수 벡터x 대신 basis로 변환한 뒤, M차원으로 확장하여 비선형 문제를 푼다.

            변환된 기저벡터의 내적과 같은 형태를 커널이라 한다.

            동일한 벡터일 때 커널은 가장 큰 값을 가지며, 두 벡터간의 거리가 멀수록 작아진다.

            * 선형 커널

            * 다항 커널 : 커널이 벡터늬 내적으로 정의됨. 하이퍼 파라미터는 γ, θ, d

            * RBF(Radial Basis Function) 커널 : 가우시안 커널이라고도 한다. 하이퍼 파라미터는 γ

                - 하이퍼파라미터는 C(Cost)와 gamma이다.

                - C는 제약조건의 강도로 C가 높다면 제약조건이 작고, C가 낮다면 제약조건이 크다.

                ex) C가 작다면 가중치 조절에서 그 값을 0에 가깝게 해 데이터에 영향이 적도록.
                오류를 허용하는 정도라고 생각해도 된다. -> 코스트가 크다면 이상치의 존재 가능성을
                작게 잡아 세심한 결정 경계를 찾아낸다.
                그렇게 되면 거의 모든 데이터가 서포트벡터(SV)로 지정이 된다.
                C가 크다면 몇몇의 데이터만 선택적으로 서포트벡터로 지정된다.

                - gamma는 가까이 있는 데이터의 가중치를 어떻게 둘 것인가.

                ex) 감마가 클수록, 좁은 영역에서 큰 점수를, 감마가 작을수록 넓은 영역까지 점수를 부여.
                감마가 크게 되면 트레인 데이터에 대해서는 잘 맞게 되나, 너무 편향될 수 있을 것 같다.
                50이상, 100과 같이 되면 너무 편향되는 경향이 있다.


            * 시그모이드

---------------------------------------------------------------------------------------------------

RBF의 튜닝

c_values = [1, 5, 10, 50, 100]
gammas = [1, 10, 100, 1000]
kernel_values=['rbf']

from sklearn.model_selection import GridSearchCV

param_grid=dict(C=c_values, kernel=kernel_values, gamma = gammas)
model = SVC()
kfold = KFold(n_splits=num_folds, random_state=seed)
grid = GridSearchCV(estimator = model, param_grid=param_grid, scoring=scoring, cv=kfold)
grid_result = grid.fit(X_train, y_train)

print("Best: %f using %s" %(grid_result.best_score_, grid_result.best_params_))

means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']

for mean, stdev, param in zip(means, stds, params) :
    print("%f (%f) with: %r" % (mean, stdev, param))

---------------------------------------------------------------------------------------------------

    3) LDA(선형판별분석법)

        - 대표적인 확률론적 생성모형. 가능도 y의 클래스 값에 따른 x의 분포에 대한 정보를 얻고,
        베이즈 정리를 사용하여 주어진 x에 대한 y의 확률분포를 찾아낸다.

        - 데이터를 선형으로 분류가 가능하도록 투영시키는 알고리즘.

        - PCA도 벡터를 투영시키는데, 이때 투영들의 분산이 큰 벡터를 찾는다. 그리고 분산을 적게
        만드는 벡터들은 제거해줌으로서 차원을 감소시킨다.
        -> 고차원의 데이터를 차원축소를 통해 시각화가 가능하게 하거나 노이즈를 제거하는 것이 목적

        - 반면 LDA는 데이터를 하나의 선으로 투영. 데이터가 속하는 클래스를 알아야 한다.

        - 클래스 간의 거리는 최대가 되어야 하며, 클래스 내 데이터들의 분산은 최소가 되어야 한다.

        -> 따로 하이퍼 파라미터가 없다!

---------------------------------------------------------------------------------------------------

    4) KNN(K-Nearest Neighbors)

        - 분류와 회귀에 모두 작동한다.

        - 예측하려는 데이터와 input 데이터들간의 거리를 측정하여 가장 가까운 K개의 데이터셋 레이블을
        참조해 분류/예측한다.

        - 학습시에 단순히 input 데이터들을 저장만 하여 거리를 계산한다.
        학습은 빠르지만 예측시에 시간이 많이 걸린다.

        - 하이퍼파라미터는 K로 분류시에 확인할 주변 데이터포인트의 수를 지정한다.
        K가 너무 크면 과적합(Overfitting), 작으면 성능이 나빠진다.(underfitting)

        - 거리를 재는 방식

            p = 1 : 맨하탄거리
            p = 2 : 유클리디안 거리(default)

        - 복잡한 알고리즘 보다는 baseline을 잡기위한 확인용 알고리즘.

//////////////////////////////////////////////////////////////////////////////////////////////////////////

앙상블 모델

    - 머신러닝을 위해 다양한 학습 알고리즘을 결합시켜 학습시키기 때문에 예측력도 향상시키고 단점 또한 보완된다.

    - 배깅(bagging), 부스팅(boosting), 스태킹(stacking) 기법이 있다.

    * Bootstrap은 통계학 용어로, 재추출(random sampling)을 적용하는 방법을 일컫는다.

        -> 표본을 통해 모집단의 데이터를 얻을때, 표본에 대한 복원추출을 여러번 시행하여 실제 모집단의 값과 유사한 값을 얻을 수 있다.
        -> 마찬가지로 머신러닝에서도 random sampling을 통해 training data를 늘릴 수 있다.

---------------------------------------------------------------------------------------------------

배깅(Bagging) - Bootstrap aggregating(부트스트랩을 병렬 복원추출로 집계)

    - 부트스트랩을 집계하여 학습 데이터가 충분하지 않더라도 충분한 학습 효과를 주어 높은 편향의 underfitting 문제나
    높은 분산으로 인한 overfitting 문제를 해결하는데 도움을 준다.

    - 대표적인 예로 랜던 포레스트(RandomForest)가 있다.

---------------------------------------------------------------------------------------------------

부스팅(Boosting) - 순차적인 복원추출을 통해 가중치를 준다.

    - 배깅과 거의 동일한 매커니즘을 가지고 있다. 다른점은 병렬적으로 집계하는 것이 아닌 순차적인 학습의 진행.

    - 배깅은 각 분류기들이 학습시에 상호영향을 주지 않고 독립적으로 끝나지만, 부스팅은 모델을 개선해 나가는 방향으로 학습이 진행된다.

    - 일반적으로 오답에 대해 높은 가중치를 부여하여 정확도는 높지만, 이상치에 취약한 단접이 있다.

    - 대표적으로 AdaBoost, GradientBoost, XGBoost가 있다.

    1) 아다부스트(Adaboost) - adaptive boosting

        약한 분류기들이 상호보완 하도록 순차적으로 학습시켜 성능을 증폭시킨다.

        약한 분류기들은 하나씩 순차적으로 학습시킬 때, 먼저 학습된 분류기가 잘못 분류한 결과 정보를 다음 분류기의 학습 시 사용하여
        단점을 보완한다.

    2) Gradient Boosting

        강력한 모델이지만 느리고 과적합될 가능성이 있다.

    3) XGBOOST

        gbm 보다는 빠르며, 과적합 방지에 대한 규제가 있다.

        CART(Classification and regression tree)를 기반으로 하기 때문에 분류와 회귀에서 모두 사용가능하다.

        조기종료를 제공한다.

        가중치 부여시에 경사하강법(gradient descent)을 사용한다.

        하이퍼 파라미터는 다음과 같다.

            n_estimators - 결정트리의 개수

            max_depth - 트리의 깊이

            colsample_bytree - 컬럼의 샘플링 비율

            subsample - 학습에 사용하는 데이터의 샘플링 비율

            learning_rate - 학습률

            min_split_loss - 리프 노드를 추가적으로 나눌지 결정하는 값

            reg_lambda - L2 규제

            reg_alpha - L1 규제

        -> 일반적인 모델링 과정에서 사용하는 grid search를 통해 하이퍼파라미터를 튜닝해 준다.

        모델이 무겁고 시간이 오래 걸리므로 튜닝을 너무 많이 시키지 않는 것이 좋을 것 같다.

from xgboost import plot_importance
from xgboost import XGBClassifier

xgb = XGBClassifier()

xgb_param_grid = {'n_estimator':[100,200,400,600], 'learning_rate':[0.01,0.05,0.1,0.15,0.2], 'max_depth':[4,6,8,10,12]}

xgb_grid = GridSearchCV(xgb, param_grid = xgb_param_grid, scoring='accuracy', cv = kfold)

## n_jobs = -1로 설정하면 모든 코어를 사용. 과부화될지도..?
## verbose 를 통해 log 출력의 level을 조정해준다. 숫자가 클수록 많은 log 출력

xgb_grid.fit(X_train, y_train)

print("최적의 정확도 :{0:.4f}".format(xgb_grid.best_score_))
print("최적의 파라미터 : ", xgb_grid.best_params_)

cv_result_df = pd.DataFrame(xgb_grid.cv_results_)
cv_result_df.sort_values(by=['rank_test_score'], inplace=True)
cv_result_df[['params','mean_test_score','rank_test_score']].head(10)

xgb = XGBClassifier(n_estimator = , learning_rate = , mac_depth = )
xgb.fit(X_train, y_train, early_stopping_rounds = 100)

#조기종료를 100으로 설정하면 100번동안 성능이 좋아지지 않으면 조기종료한다.-> 시간낭비를 줄여준다.
#eval_set에는 검증세트를 지정해 준다.
#eval_metric은 평가방식으로 rmse, mae, logloss등이 있다.

xgb_pred = xgb.predict(X_test)

metrics(y_test, xgb_pred)

fig, ax = plt.subplots()
plot_importance(xgb, ax=ax)






















--------------------------------------------------------------------------------------------------

스태킹(Stacking) - 교차검증을 기반으로 서로 상이한 모델들을 조합한다.

    - 개별적인 모델이 예측한 데이터를 다시 training data set으로 활용하여 학습























