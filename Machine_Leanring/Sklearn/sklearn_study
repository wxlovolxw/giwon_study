
사이킷런 설계 철학

    일관성 - 모든 객체가 일관되고 단순한 인터페이스를 공유

    추정기(estimator) - 데이터셋을 기반으로 일련의 모델 파라미터들을 추정하는 객체를 추정기라 한다.
                    SimpleImputer도 하나의 추정기.
                    추정은 fit() 메서드에 의해 수행되며, 하나의 매개변수로 하나의 데이터셋만 전달한다.
                    지도학습시에는 매개변수가 두개로 두번째는 레이블(Y)를 담고 있다.

    변환기(transformer) - 데이터셋을 변화하는 역할을 하는 추정기를 변환기라고 한다.
                    데이터셋을 매개변수로 전달받은 transform() 매서드가 수행한다.
                    그리고 변환된 데이터셋을 반환한다.

                    * fit과 transform을 연달아 호출하는 fit_transform()을 사용하도록 하자.

    예측기(predictor) - 일부 추정기는 주어진 데이터셋에 대해서 예측을 만들 수 있다.
                    회귀 모델등이 그러한 예측기이다.
                    예측기는 predict() 메서드는 데이터셋을 받아 이에 상응하는 예측값을 반환.
                    또한 테스트셋을 통해 예측의 품질을 측정하는 score() 메서드를 갖는다.

    검사가능 - 공개Publid 인스턴스 변수로 직접 접근 가능.
            모델 파라미터도 접미사로 밑줄을 붙여서 공개 인스턴스 변수로 제공

            ex) SimpleImputer.statistics_

    클래스 남용 방지 - 데이터셋을 별도의 클래스가 아닌 넘파이 배열/사이파이 희소sparse 행렬로 표현

    조합성 - 기존의 구성요소를 최대한 재사용.
            여러 변환기를 연결한 후, 마지막 추정기 하나를 배치한 Pipeline 추정기를 쉽게 만들 수 있다.

    합리적인 기본값 - 기본 시스템을 빠르게 만들수 있도록, 대부분 매개변수에 합리적인 기본값이 지정.

///////////////////////////////////////////////////////////////////////////////////////////////

데이터의 준비를 자동화하는 방법

    1) 데이터의 정제(변환) -> 결측치의 처리

        - 해당구역(row)를 삭제

            df.dropna(subset=['column'])

        - 해당특성(column)을 삭제

            df.drop('column')

        - 어떤 값으로 채움(0, median, mean)

            median = df['column'].median()
            df['column'].fillna(median,inplace=True)

        * 누락값을 채울 때, 사이킷런의 Imputer를 사용하면 된다.

            from sklearn.impute import SimpleImputer

            simr = SimpleImputer(missing_values=np.nan, strategy='median')
            # 결측값의 형태(int, float, str, None 등)
            어떻게 처리할 것인지 지정(median, mean, most_frequent 등)

            train 데이터프레임에 대해서 수치형 데이터만을 따로 받아
            train_num = train.drop(["Name","Sex","Ticket","Cabin","Embarked"], axis=1)

            simr = simr.fit_transform(train_num)
            simr.statistics_
            # 주어진 SimpleImputer에 대해서 train셋을 훈련시킨다.
            그 결과는 .statistics_에 저장된다.
            이때 주어진 형태는 numpy array이므로 다시 pd.df의 형태로 만들어 준다.

            train_tr = pd.DataFrame(X, columns=train_num.columns, index = list(train.index.values))

            train_tr.isnull().sum()을 통해 확인해본 결과

                PassengerId    0
                Survived       0
                Pclass         0
                Age            0
                SibSp          0
                Parch          0
                Fare           0

            로 모든 결측치가 채워졌다.

    -----------------------------------------------------------------------------------------------

    2) 텍스트와 범주형 특성 다루기

        기존에 mapping을 통해 카테고리형 특성을 다뤘던 것과 비슷하다.
        factorize() 메서드를 사용해준다.

            train_cat = train[['Sex','Embarked']]

            -> factorize()는 데이터프레임에 대해서 작동하지 않으므로 개별의 시리즈로 따로따로 계산해줘야 한다.

            train_sex = train['Sex']
            train_emb = train['Embarked']

            train_sex_encoded, train_sex_categories = train_sex.factorize()
            train_emb_encoded, train_emb_categories = train_emb.factorize()

            # encoded에는 1차원의 배열이 형성된다.

        이런 카테고리형 데이터의 경우 문제가 발생하는데, 0과 1 사이의 거리가 0과 2보다 유사하다는 의미가 아니다.
        문제를 해결하기 위해, 벡터화하는데 이 방법이 원-핫 인코딩이다.

            from sklearn.preprocessing import OneHotEncoder
            encoder = OneHotEncoder()

            train_sex_1hot = encoder.fit_transform(train_sex_encoded.reshape(-1,1))
            train_emb_1hot = encoder.fit_transform(train_emb_encoded.reshape(-1,1))

            # fit_transform에는 2차원 배열이 들어가야 하므로 reshape를 통해 2차원 배열을 만들어준다.
            reshape에 -1은 기존 배열의 모양을 통해 추측하여 생성해준다.

        * 벡터화 하는 것은 카테고리의 수가 적을때 사용하도록 한다.
        -> 원래 저장된 형태는 0이 아닌 위치만 저장된다.

        위의 카테고리화와 원핫인코딩을 한번에 처리해주는 변환이 존재한다.

            from sklearn.preprocessing import CategoricalEncoder
            cat_encoder = CategoricalEncoder()

            train_sex_reshaped = train.sex.reshape(-1,1)
            train_emb_reshaped = train.emb.reshape(-1,1)

            train_sex_1hot = cat_encoder.fit_transform(train_sex_reshaped)
            train_emb_1hot = cat_encoder.fit_transform(train_emb_reshaped)

        -> 기본적으로 Scipy compressed sparse matrix(축약형태의 희소행렬)을 출력하므로
        밀집행렬의 형태를 원할 경우 encoding='onehot-dense'를 사용해 준다.

        .categories_를 통해 카테고리의 리스트도 얻을 수 있다.

        -> categorical encoder는 현재 사용할 수 없다. 따로따로 해줘야 할 것 같다.

        class CategoricalEncoder(BaseEstimator, TransformerMixin) :

            def __init__(self, to_categorical_onehot) :
                self.to_categorical_onehot = to_categorical_onehot

            def fit(self, X, y=None) :
                return self

            def transform(self, X, y=None) :
                X_encoded, X_categories = X.factorize()
                encoder = OneHotEncoder()

                return encoder.fit_transform(X_encoded.reshape(-1,1))

            -> 를 통해 작동하지 않는 카테고리칼인코더와 마찬가지로 카테고리화 + 원핫인코딩을 하는 클래스를 생성하였다.


    -----------------------------------------------------------------------------------------------

    3) 특성 스케일링

        머신러닝 알고리즘은 입력 숫자 특성들의 스케일이 많이 다른 경우에 잘 작동하지 않으므로 스케일링을 해준다.
        레이블에 대해서는 해줄 필요가 없고, 특성들에 대해서만 해준다.

        - min-max 스케일링(정규화)

            모든 범위의 값들이 0-1사이에 분포하도록 만든다.
            값에 최소값을 뺀 뒤, 최대값과 최소값의 차이로 나눠주면 된다.
            MinMaxScaler 변환기를 사용한다.
            feature_range 매개변수를 통해 범위를 조정할 수 있다.

        - 표준화

            평균을 뺀 후 표준편차로 나누어 스케일링 결과 분포의 분산이 1이 되도록 한다.
            상한과 하한이 없어 알고리음즤 문제가 될 수 있으나, 이상치의 영향을 덜 받는다.
            StandardScaler 변환기를 사용한다.

        * 스케일링은 전체 데이터가 아닌 훈련 데이터에 대해서만 fit() 메서드를 적용해야한다.
        그리고 훈련세트와 테스트세트(+새로운 데이터)에 대해서 transform() 메서드를 사용한다.

    -----------------------------------------------------------------------------------------------

    4) 사용자 지정 변환기

        사이킷런은 상속이 아닌 덕 타이핑을 지원하므로 fit(), transform(), fit_transform() 메서드를 구현한 클래스를 생성

        마지막 매서드는 TransformerMixin을 상속하면 자동으로 생성.
        BaseEstimator를 상속하면 하이퍼파라미터 튜닝에 필요한 두 메서드(get_params(), set_params())를 얻게 된다.

        ex) 기존의 컬럼들을 합쳐 새로운 컬럼을 만드는 변환기

            from sklearn.base import BaseEstimator, TransformerMixin

            sibsp_ix, parch_ix = 4,5

            class CombinedAttributesAdder(BaseEstimator, TransformerMixin) :

                def __init__(self, add_family_size=True) :
                    self.add_family_size = add_family_size

                def fit(self, X, y=None) :
                    return self

                def transform(self, X, y=None) :
                    family_size = X[:,sibsp_ix] + X[:,parch_ix] + 1
                    return np.c_[np.delete(X,[4,5],axis=1),family_size]

                # X는 2d array인 반면 family_size는 1d array이기 때문에 np.concatenate(,axis=1)을 통해서는 연산이 안된다.
                np.c_[A,B]의 형태로 바꾸니 해결되었다.

            attr_adder = CombinedAttributesAdder(add_family_size=True)

            train_extra_attribs = attr_adder.transform(train_num.values)


        ex) 필요한 특성(컬럼)을 선택하여 데이터프레임을 넘파이 배열로 바꿔주는 변환기

            from sklearn.base import BaseEstimator, TransformerMixin

            class DataFrameSelector(BaseEstimator, TransformerMixin) :

                def __init__(self, attribute_names):
                    self.attribute_names = attribute_names

                # 해당 매개변수를 속성에다가 저장.

                def fit(self, X, y=None) :
                    return self

                def transform(self, X) :
                    return X[self.attribute_names].values

    -----------------------------------------------------------------------------------------------

    5) 변환 파이프라인인

        변환 단계가 많고 정확한 순서대로 처리하도록 도와주는 Pipeline 클래스가 있다

            from sklearn.pipeline import Pipeline
            from sklearn.preprocessing import StandardScaler

            num_attribs = list(train_num)
            cat_attribs = ["Sex","Embarked"]

            num_pipeline = Pipeline([
                ('selector', DataFrameSelector(num_attribs)),
                ('simple_imputer', SimpleImputer(strategy="median")),
                ('attribs_adder', CombinedAttributesAdder()),
                ('std_scaler', StandardScaler())
            ])

            cat_pipeline = Pipeline([
                ('selector',DataFrameSelector(cat_attribs)),
                ('cat_endocer',CategoricalEncoder(to_categorical_onehot=True))
            ])

        별개의 두 파이프라인을 합치기 위해 FeatureUnion을 사용한다.

            from sklearn.pipeline import FeatureUnion

            full_pipeline = FeatureUnion(transformer_list=[
                ("num_pipeline", num_pipeline),
                ("cat_pipeline", cat_pipeline)
            ])

///////////////////////////////////////////////////////////////////////////////////////////////

    class CategoricalEncoder(BaseEstimator, TransformerMixin) :

        def __init__(self, to_categorical_onehot) :
            self.to_categorical_onehot = to_categorical_onehot

        def fit(self, X, y=None) :
            return self

        def transform(self, X, y=None) :

            encoder = OneHotEncoder()
            for i,cat in enumerate(cat_attribs) :

                X_encoded, X_categories = pd.Series(X[:,i]).factorize()
                X_cated[:,i] = encoder.fit_transform(X_encoded.reshape(-1,1))

            return X_cated

    train_cat = cat_encoder.transform(train[cat_attribs].values)

















